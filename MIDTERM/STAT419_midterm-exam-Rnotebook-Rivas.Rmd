---
title: 'R Notebook midterm: 419 Survey of Multivariate Methods'
name: "Kathleen Rivas"
email: "kathleen.rivas@wsu.edu"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    toc_depth: 6
    fig_caption: true
    number_sections: false 
params:
  knitChunkSetEcho: TRUE 
  knitChunkSetWarning: FALSE
  knitChunkSetMessage: TRUE
  knitChunkSetCache: TRUE
  knitChunkSetFigPath: "graphics/"
    
my-var: "monte"  # https://bookdown.org/yihui/rmarkdown/html-document.html
---
# Top of the world
<https://brand.wsu.edu/visual/colors/>
```{r setup, include=FALSE}
# I am now setting parameters in YAML header, look above
knitr::opts_chunk$set(echo = params$knitChunkSetEcho);
knitr::opts_chunk$set(warning = params$knitChunkSetWarning);
knitr::opts_chunk$set(message = params$knitChunkSetMessage);

# ... just added ... take a look at how this builds ... you now have your raw files ...
knitr::opts_chunk$set(cache = params$knitChunkSetCache);
knitr::opts_chunk$set(fig.path = params$knitChunkSetFigPath);

###########################
options(scipen  = 999);

library(devtools);
library(humanVerseWSU);
packageVersion("humanVerseWSU");  # ‘0.1.4.2’+
path.github = "https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/";
path.mshaffer = "http://md5.mshaffer.com/WSU_STATS419/";

source_url( paste0(path.github,"misc/functions-midterm-F2000.R") );  # should be 2020 ... oh well

source_url( paste0(path.github,"humanVerseWSU/R/functions-EDA.R") );  # EDA functions ...


library(parallel);
parallel::detectCores(); # 16 # Technically, this is threads, I have an 8-core processor 

```
## TESTING PROCEDURE
 
This midterm exam (Rnotebook-midterm) is worth 150 points.  For each question, review how much the item is worth in terms of points and plan your time wisely. 

I would deem it "unwise" to spend hours on a question that is only worth 5 points.

### Static/Existing Resources Are Allowed
 
This is an open-book examination.  You can use your course notebooks (digital and old-school).  You can use Internet resources (stackoverflow, Wikipedia, and Youtube).  

### Dynamic/Living Resources Are _ **NOT**_  Allowed

**You cannot use a living resource on the exam.**  That would include a classmate, student, sibling, parent, a tutor, online forums (where you ask the question after the exam period has begun). 

If you have questions that need clarifying, please **email the instructor** and he will try to answer them by email or ZOOM.  He will be checking his email often during the week of the exam, to make himself available to you.

### Levels of Mastery

* Do You **Remember**?
* Do You **Understand**?
* Can You **Apply** what you remember/understand to another similar problem?
* Can You **Analyze** and **Synthesize** Data?
* Can You **Evaluate** your analyses?
* Can You **Create** meaningful visualizations and summaries?  

### Rubric of Mastery 
 
For every 10 points, this is the general breakdown.
 
| Emerging       | Developing     | Mastering    |
| :------------- | :----------:   | -----------: |
| 0-4            |  5-7           | 8 - 10       |



<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">

# EXPLORATORY DATA ANALYSIS (EDA)

Exploratory data analysis (EDA) is the process of analyzing data to summarize its main characteristics.

Confirmatory data analysis (CDA) is the process of applying specific statistical methods to analyze the data.  The goal is also to summarize its main characteristics.  We commonly refer to CDA as "statistical hypothesis testing".  CDA generally makes assumptions about how the data is distributed.  Or wants to apply a specific model to the data.

So the two approaches have the same objective:  summarize the main characteristics of the data.  How they achieve that objective is very different.  

## Introduction 

### John Tukey
John Tukey is the father of "Exploratory Data Analysis" (EDA) <https://en.wikipedia.org/wiki/John_Tukey>.  My favorite statistics book is his 1977 book (not surprising) entitled "Exploratory Data Analysis."

### Exploratory vs Confirmatory
From Wikipedia (Accessed October 2020):  Tukey "also contributed to statistical practice and articulated the important distinction between exploratory data analysis and confirmatory data analysis, **believing that much statistical methodology placed too great an emphasis on the latter**."

I belong to the "Tukey" camp. I believe too much emphasis is place on "formal statistical methods and tests".  I believe more emphasis should be placed on the underlying nature of the data.  These underlying principles are how the statistical methods developed.  

As a data analyst, I believe that first and foremost, we should let the data speak.  That is why the first half of the semester started in this form.  The second half (confirmatory data analysis) will rely on what is labeled by many as "formal statistical methods".

### Tukey and "Bell Labs"

In 1965, Tukey divided his time between working at Princeton University and working at Bell Labs (a research think tank).  

#### Robust Statistics as Nonparametric

Tukey proposed that five summary data are essential to understanding numerical data:  `min`, `max`, `median` (technically `Q2`), and `Q1` and `Q3` (the quartiles).  In `R`, the function `summary` has only added `mean` to Tukey's proposal from years ago.

#### Box and Whisker Plot 

In 1975, Tukey invented the "box and whisker" plot that identifies the median, inter-quartile range (IQR), and outliers of data.  The visualization displays the data without making any assumptions about its statistical distribution.  The boxplot is a working demonstration of EDA.  **Let the data speak!**

### John Chambers and `S` and `R`
At the same time ast John Tukey, three other men were also working at Bell Labs (John Chambers, Rick Becker, and Allan Wilks) on a statistical programming language `S` that emphasized EDA.  This "statistical computing" language was programming mostly in `Fortran` with some `C` programming.    Chambers published his first "statistical computing" text in 1977, titled "Computational methods for data analysis" <https://archive.org/details/computationalmet0000cham/page/n11/mode/2up>

Between 1988 and 1991, Chambers updated the engine of `S` to make it more robust.  That same engine still powers much of `R` today.  That is, much of the base code of `S` was written by Chambers himself. `R` today still uses much of that `S` codebase under the hood.  

`R` was an open-source offshoot (a "fork") of `S` which occurred in the mid 1990s.  Today, Chambers is still active in the `S`-now`R` community.  My favorite book of his is titled (2008): "Software for data analysis programming with R".  My second-favorite book of his is titled (1998): "Programming with data: a guide to the S language".  In 2016, he authored another book that I still need to read "Extending R."  

Modern `R` is written in `Fortran`, `C`, and `C++`.  

Since its foundation is primarily `C`, we can use standard "make" and "make-install" tools to compile R or its packages from the source code.  That is why we needed `Rtools` on Windows.  The MacOS is now linux based, so no additional tools are required.

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

## Summary

EDA as exploration is an iterative process.  

### Analogy: learning a foreign language
I like to use the analogy of learning a foreign language using the "immersion" approach. For example, I studied Spanish in high school, learned vocabulary and grammar, and really could not speak the language well.  

I did learn to speak the language well by being dropped into a foreign country for nearly two years.  Some key ingredients to learn a language in an "immersive" environment are listed below:  

- surround yourself with others speaking the language to be learned [e.g., I did not spend a lot of time with other Americans speaking English]. 
- be present when engaged in the language.  Listen intently and try to understand as much as you can, not worrying about what you don't fully understand.  

- reflect after language engagement.  Try to synthesize what "gaps" you have and then develop study habits to fill in those gaps.

- practice what you have learned.

To some degree, my success was likely accelerated because I had precursory training.  Regardless, "immersive" practices benefit learning new languages.

### Proficiency in Data Analytics

[As part of your journey, I have asked you to keep a "paper-and-pencil" notebook to write down words/phrases/ideas.  For example, in this section, there may be words/terms/phrases/ideas you don't fully understand.]

Proficiency requires an iteration of these key features described above.  But first, you have to understand what language you are trying to speak.  Is it `R`?  Is it Statistics?  Mathematics?  What exactly is the language?

In my opinion, the language is the "language of data". 

### The "language of data"

How do you think mathematics developed?  It likely started with simple data, based on real-world experience:  two hands, five fingers on each hand gives me the number ten.  Counting in a base-10 system likely resulted.  An entire domain of mathematics called "number theory" devotes its studies to these integer values.

How do you think statistics developed?  People went out and started measuring things.  One person would literally walk down the street in the late 1800s and ask if he could measure a person's proportions.  Another person would study crop yields at different locations and tried to ascertain if they were different. 

The foundation of mathematics and statistics is data. So I believe, we should let the data speak.

### Let the data speak

So I am definitely an EDA-guy.  Some people are, some people are not.  I personally am a strong believer that we should **let the data speak**, learn how to describe the data without imposing any restrictions on it, and always think about the data first and foremost.  

I also believe that we should use logic, intuition, and insight before we develop any formal "confirmatory" hypothesis testing.  I have intentionally architected this course to emphasis EDA.

### Quality data provenance

I am also very ademant about **data provenance** as I believe the "outputs" of any analysis (whether exploratory or confirmatory) is as only as good as the data quality.  I call this `data intimacy`.  You should care just as much about the process to get quality data as you do to analyze said data.

The term **GIGO** (garbage-in, garbage-out) in my estimation represents what happens when care for quality data is treated lightly.  

### Iterative Exploration
This full EDA approach is a multi-lens approach.  View the data from as many different perspectives as possible before arriving at a conclusion.  Base your conclusion on a synthesis of what you analyzed from those different perspectives.

- We do initial EDA (using mathematical foundations), 
- then we may do confirmatory analyses (traditional statistical methods), and 
- then we synthesize our findings and do a higher-ordered EDA using the original analysis and the confirmatory analysis to make final decisions using sound logic and intuition.
- this process will enlighten our understand and possibly help us formulate new suppositions and think about what additional data would inform the topic.

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

## (10 points) YOUR "EDA" OPINION 
[
I have expressed my opinion about the study of data and the importance of EDA in that study.  What is your opinion on this topic?

This is worth 10 points, a minimal answer should be at least 3 paragraphs. Agreeing/Disagreeing with my opinion is not how you will be evaluated.  How well you express YOUR opinion is what is important.]

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">

In a way, the phrase "the study of data" and "EDA" are interchangeable, the way statistics can sometimes refer to the same thing in different ways -- one and two tailed vs one and two sided t-tests come to mind.  Confirmatory analysis is only a support for the study of data, i.e. EDA.  

The concept of data are measurements and numbers that people or programs/algorithms/technology gather.  In essence, it's gathered to try to glean information.  If no information can be derived from data, the data is no "good", it is "meaningless" without a human to synthesize the data into something useful-- a prediction, or insight.

This leads to the ideas of EDA, data provenance, and the human doing something with the data, all working together in the "study of data".  First and last is the human factor.  Without it, EDA and data provenance can't exist.  

The human component is the most variable component.  To be a good data analyst, you need the tools of the trade (programming, math, and statistical knowledge).  Also, you need to do proper research on context (i.e. if you are trying to derive information on coronavirus pandemic, you need to research context of pandemics and medical information regarding viruses, but also have some general knowledge regarding human psychology to understand how pandemics may spread).  Often, I forget to research context when I work on school assignments.  I'm often in a rush to "just answer the question" that I don't bother to read any contextual information on what I'm trying to plot or understand the t-value, etc.  Context deepens the analysis, and gives you a point of reference to see if the EDA you perform makes sense.  Lastly, I think that humans, especially in a work environment, often bias their analysis in support of a result a company wants (or what they think should be).    

The human component is also critical to data provenance. What is the point of generating information on "bad" data?  Let me digress with a tangentially related story on data provenance: 

My data scientist sister and brother-in-law panicked in mid-March regarding news of coronavirus cases in Californa and China and drove two days, without stopping in a hotel, to my parents' house in Dallas, Texas, where, at the time, there was no reported cases.  

I discussed with her that the news was not the best source of information.  That by the time the first cases, anywhere, were reported, Covid-19 would have been spreading for weeks, if not months.  The spread of the virus was inevitable, due to humans travelling -- and that it would hit major cities first and make their way to smaller regions (unless, perhaps, it was super remote with very little travel in the area...which Dallas is not).  

She agreed that the news was not the best source of information, but it was the only information she had, and she would make the best decision she could given what information she had.  

I get it, but also don't get it -- because sometimes "bad" data is all you have.  However, one shouldn't wholly rely on data that is not quality.  Or make some conclusions, but with a grain of salt. Fast forward to 3-4 months later, with Texas cases skyrocketing past California, I told my sister that maybe she should have stayed in California, as it is clearly a "safer" city to be in when compared to shelter-in-place measures, with case count, etc.  Her response was, "It was bad data. I made the right decision on bad data."  

As if it was the data's fault.  It was the human's fault for generating decisions on poor data!  

Last month, my sister sheepishly told me she and her husband may have overreacted by going to Texas, and leaving the apartment in a state of chaos -- she was also one of the people guilty of buying so much toilet paper, sanitizer, clorox wipes, etc. It was good for me because I didn't buy toilet paper and got concerned when our supply became low.  Her supply lasted us until the toilet paper shortage ended, and then some. 

-----

I realize that this anecdote does not strictly correlate to EDA or data provenance, but I do think that high level, this reflects GIGO.

All three: data provenance, objectivity, and EDA, are needed in the study of data. I argue that without objectivity, quality EDA or data provenance is possible. Objectivity involves more than an effort to remain unbiased on an intellectual level -- human emotions can affect objectivity.  I argue that my sister, in her fear, used a lack of judgement/objectivity, and accepted less than quality data (poor data provenance) to make poor analysis (poor EDA) that resulted in an incorrect prediction/insight. 

In conclusion, I believe in data provenance strongly, and the role of EDA, which is an unbiased exploration of data, in an iterative process, which at the end of the process, generates meaningful information in the form of insights and/or predictions.  Confirmatory analysis has shades of bias, because, as stated, it "generally makes assumptions about how the data is distributed"...assumptions are very similar to bias.  

</pre>

<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">

## SIMULATING DATA 

### (10 points) Basic Simulation 

- Pick a `set.seed` choice so the code is replicable.  Verify that every time you run the commands, the data is not changing with the seed "you chose".
- Use the functions `rnorm`, `runif` to simulate data.  
- Simulate `n=9999;` data for each.  
- Call `x.rnorm` the data for the first and `x.runif` the data for the second.  
- Plot a histogram `graphics::hist` and report the summary statistics ``base::summary` of each. 
- Then, plot them using `plot(x.rnorm, x.runif);`.  
- Finally, `plot(x.rnorm, sample(x.rnorm) );` and compare it to `plot(x.runif, sample(x.runif) );`.  


#### Code of simulation
```{r, chunk-simulating-rnorm-runif, cache.rebuild=TRUE}

## your code goes here ...
set.seed(1)
n=9999

x.rnorm = rnorm(n, 0, 1)
x.runif = runif(n, 0, 1)

graphics::hist(x.rnorm, plot=TRUE)
graphics::hist(x.runif, plot=TRUE)

base::summary(x.rnorm)
base::summary(x.runif)

plot(x.rnorm, x.runif)
plot(x.rnorm, sample(x.rnorm))
plot(x.runif, sample(x.runif))
```

#### **Describe `rnorm`, `runif`**

- Describe what each function `rnorm` and `runif` does. How are they similar?  How are they different?  
- What does the `sample` function do?
- How was `plot(x.rnorm, x.runif);` different from `plot(x.rnorm, ( x.rnorm.sample = sample(x.rnorm) ) );` and `plot(x.runif, ( x.runif.sample = sample(x.runif) ) );`?  How would you describe the shape of each of these plots?

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
rnorm: generates random values that will follow a normal distribution. n = quantity, 0 = mean, 1 = sd.
runif: generates random values that will follow a uniform distribution.  n = quantity, 0 and 1 sets the range.

They are similar in that R will generate random values for you, and different because you are using a different distribution that these values will have as a whole.

The histograms show the distribution difference - rnorm produces the normal bell-curve distribution, and runif (which until today, I thought of as "run-if" instead of "r-unif") produces the uniform straight-across distribution shape.

sample: This function allows you to randomly select from the vector. If no size is specified, it basically scrambles the vector randomly.  If a size is specified, it acts more like a sample, only giving you subset of randomly selected values.  (You can also replace or not, and set probability --default is null)

plot comments: First plot (rnorm x runif) is columnar; second plot (rnorm x sample rnorm) is a centered oval; third plot (runif x sample runif) covers the entire quadrant (uniformly). Runif is uniform distribution from 0 to 1, which is why the first plot is densest in between those values.  
</pre>

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

### (5 points) "Easter-Egg" Simulation 

- There was an "Easter Egg" that related to setting the seed `set.seed` using `rbinom`.  If you search in the BlackBoard discussion forum for `easter` you will see the discussion about August 25-27.
- In the "Easter Egg", the goal was to find a scenario using a specific `set.seed` that would simulate flipping a coin 100 times and getting one result (heads/tails) exactly 52 times. 
- In this problem, the search criteria has changed.  Simulate flipping a coin 1000 times and getting one result (heads/tails) exactly 555 times. 
- You need to report 5 values for `set.seed` that achieves this objective.  You can report more.  
- You should explicitly have the code print `length(x)` where `x` is a vector of the values that meet the objective.


```{r, chunk-simulating-rbinom, cache.rebuild=TRUE}

## your code goes here ...

```


<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- OPTIONAL WRITING ... CODE SHOULD BE SELF EXPLANATORY --
</pre>




<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

### Rolling the Dice

- You have 3 dice.
- Each dice has the numbers `1:10` ... they are ten-sided die ("decader" die).
- Write the necessary `for-loops` to capture all possible outcomes of rolling the three dice at the same time.
- A dataframe `myrolls` should have three columns: `dice.1`, `dice.2`, `dice.3` plus a fourth column `roll.total` which is the sum `dice.1 + dice.2 + dice.3` of one iteration of the nested `for loop`.
- Report the dimensions `dim` of `myrolls`.
- Create a table `outcomes.table` that summarizes the counts of the `myrolls$roll.total` 
- Transform the table to a dataframe `outcomes.df`. Name the columns: c("roll.total", "count");
- Report the sum of `outcome.df$count`
- Create a new column `outcomes.df$prob` (Probability) that determines the probability of that row given the total sum of the `count` column.
- Display the dataframe.

#### Setting Up the Dice Scenario 
```{r, chunk-simulating-dice, cache.rebuild=TRUE}

## your code goes here ...

dice.1 = dice.2 = dice.3 = 1:10;

myrolls = NULL;
for(d1 in dice.1)
  {
  for(d2 in dice.2)
    {
    for(d3 in dice.3)
      {
      roll.total = d1 + d2 + d3;
      row = c(d1, d2, d3, roll.total);
      myrolls = rbind(myrolls, row);
      }
    }
  }

myrolls = as.data.frame(myrolls);
colnames(myrolls) = c("dice.1", "dice.2", "dice.3", "roll.total");

myrolls;
print("Dimensions of myrolls");
dim(myrolls);

outcomes.table = table(myrolls$roll.total);
outcomes.df = as.data.frame(outcomes.table);
  colnames(outcomes.df) = c("roll.total", "count");


total.sum = sum(outcomes.df$count);
print("Sum of outcomes.df$count");
total.sum;

outcomes.df$prob = outcomes.df$count / total.sum;
outcomes.df;

```
#### Viewing a subset of data to answer a question  

- How many ways can I roll a 23 when I throw the dice at the same time?  What is the probability that I roll a 23 on a single throw?

```{r, chunk-simulating-dice-subset-a, cache.rebuild=TRUE}
# if you don't have the latest version of humanVerseWSU, you can access the function as follows:
# library(devtools);
# source_url(paste0( path.github, "humanVerseWSU/R/functions-dataframe.R" ));

sub.myrolls = subsetDataFrame(myrolls, "roll.total", "==", 23);

sub.myrolls;


sub.outcomes.df = subsetDataFrame(outcomes.df, "roll.total", "==", 23);

sub.outcomes.df;


```
 

#### (10 points) Questions from the Dice simulation 

##### **Roll 23**
- How many ways can I roll a 23 when I throw the dice at the same time?  What is the probability that I roll a 23 on a single throw?

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">

36 ways to roll a 23. 
.036 probability I roll a 23 on a single throw.
</pre>

##### **Roll 12 or 22** 
- What is the probability that I roll a 12 or a 22 on a single throw?

```{r, chunk-simulating-dice-subset-b, cache.rebuild=TRUE}

## your code goes here ... if necessary

```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
.055 + .045 = .1
</pre>


##### **Roll 26 or 29** 
- What is the probability that I roll a 26 or a 29 on a single throw?

```{r, chunk-simulating-dice-subset-c, cache.rebuild=TRUE}

## your code goes here ... if necessary

```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
.015 + .003 = .018
</pre>

##### **Roll 3 once/twice in a row** 
- What is the probability that I roll a 3 on a single throw?  What is the probability that I roll a 3 twice in a row?  First throw = 3 **AND** second throw = 3?

```{r, chunk-simulating-dice-subset-d, cache.rebuild=TRUE}

## your code goes here ... if necessary

```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">

Probability of 3 on a single throw: .001
Probability of 3 twice in a row: .001 * .001 = .000001

</pre>

##### **Roll 12 or lower** 

- What is the probability that I roll at most a 12 on a single throw?  That is, a 12 or lower ...

```{r, chunk-simulating-dice-subset-e, cache.rebuild=TRUE}

## your code goes here ... if necessary
sub.outcomes.zero_to_twelve.df = outcomes.df[1:10,];

cumsum(sub.outcomes.zero_to_twelve.df["prob"])

```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">

.22

</pre>


<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">

## INITIAL EXPLORATION OF REAL DATA

We are going to use exploratory techniques to examine some Indeed.com data.  If you recall, this `job` data examines how many jobs reference a certain keyword.  Every Monday morning at 12:00:00AM EST (using a scheduler `crontab`), this data collection is performed.  A few weeks ago, I added some new keys words.  The data set we have consists of 5 weeks:  `2020-38` to `2020-42`.

- For each "search phrase", I go to Indeed.com and download the first page of results.  
- From this first page, I grab the "total count"
- An example is shown in a screenshot, taken this week. 

<IMG src="http://md5.mshaffer.com/WSU_STATS419/_images_/Big-data.png" style="border: 2px black solid;" />
<div>**Source: Data provenance history**</div>


<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

### Import "jobs" data
- Run code to import the data `jobs`.

```{r, chunk-plotting-load-jobs, cache.rebuild=TRUE}
jobs = utils::read.csv( paste0(path.mshaffer, "_data_/indeed-jobs.txt"), header=TRUE, quote="", sep="|");

colnames(jobs) = c("year.week", "search.query", "job.count");
jobs;
```

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

- Create a `hist` and `boxplot` and report `summary(jobs$job.count)`.

### (5 points) Histogram and Box Plot 
```{r, chunk-plotting-jobs-boxplot, cache.rebuild=TRUE}


## your code goes here 
graphics::hist(jobs$job.count, plot=TRUE)
graphics::boxplot(jobs$job.count, plot=TRUE)
base::summary(jobs$job.count)


```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">

Histogram tells me that the vast majority of job counts are 50,000 or less.  Boxplot is hard to read, with the median line at the bottom of the IQR box.  There are a few outliers -- the most popular search query keywords. Summary statistics tells me that there are weeks where there are no search queries for a particular keyword, with the max number of times a keyword shows up in a week is 404527 times!.  The median and mean are not close together, which means that the outliers are skewing the mean.
</pre>
[What does the histogram tell you about the data?  What does the boxplot tell you about the data?  What does `summary` tell you about the data?]

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### Subset some keywords relevant to this course
```{r, chunk-plotting-jobs-subset, cache.rebuild=TRUE}

deep.dive = c("Microsoft Office", "C++", "SQL", "Computer Science", "Python", "Java", "Statistics", "Data analysis", "Data analytics", "Javascript", "machine learning", "Git", "Tableau", "Business intelligence", "PHP", "Mysql", "MariaDB", "SAS", "SPSS", "Stata",  "Data entry", "Big data", "Data science", "Power BI");  # I intentionally do not include "R" because it is return irrelevant results, I should have searched for "R programming" or "R statistics" ... which I am now doing...


or = "";
for(search in deep.dive)
  {
  or = paste0(or, " jobs$search.query == '",search,"' | ");
  }
or = substr(or,0, strlen(or) - 2);

## TODO ... update subsetDataFrame to allow "OR" logic, currently only does "AND" ...

jobs.subset = jobs[ or , ];  # doesn't work ...
jobs.subset = jobs[ jobs$search.query == 'Microsoft Office' |  jobs$search.query == 'C++' |  jobs$search.query == 'SQL' |  jobs$search.query == 'Computer Science' |  jobs$search.query == 'Python' |  jobs$search.query == 'Java' |  jobs$search.query == 'Statistics' |  jobs$search.query == 'Data analysis' |  jobs$search.query == 'Data analytics' |  jobs$search.query == 'Javascript' |  jobs$search.query == 'machine learning' |  jobs$search.query == 'Git' |  jobs$search.query == 'Tableau' |  jobs$search.query == 'Business intelligence' |  jobs$search.query == 'PHP' |  jobs$search.query == 'Mysql' |  jobs$search.query == 'MariaDB' |  jobs$search.query == 'SAS' |  jobs$search.query == 'SPSS' |  jobs$search.query == 'Stata' |  jobs$search.query == 'Data entry' |  jobs$search.query == 'Big data' |  jobs$search.query == 'Data science' |  jobs$search.query == 'Power BI'  , ];

stem(jobs.subset$job.count);
subsetDataFrame(jobs.subset, "search.query", "==", "Microsoft Office" );

##my code below so my histogram looks cleaner -- I wanted to see the individual
##keywords matched up to their count.  
reorg.df = NULL
for (keyword in deep.dive)
{
  group = subsetDataFrame(jobs.subset, "search.query", "==", keyword)
  total = sum(group[,'job.count'])
  row = c(keyword, total)
  reorg.df = rbind(reorg.df, row)
}

reorg.df = as.data.frame(reorg.df)
colnames(reorg.df) = c("search.query", "job.count")
```


#### Histogram and Box Plot of Subset
```{r, chunk-plotting-jobs-boxplot-subset, cache.rebuild=TRUE}
## your code goes here 
#graphics::hist(as.numeric(reorg.df$job.count), col='skyblue3', breaks=c(24))
graphics::hist(as.numeric(reorg.df$job.count), col='skyblue3')
graphics::boxplot(as.numeric(reorg.df$job.count))

```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">

The histogram tells me that the there are only a few search.query keywords that are massively popular -- about 2 that had 800,000+ in the last 5 weeks. The majority of them (15) are in the 0 - 200,000 bin. 

The boxplot shows that the median is close to Q1 (25th percentile). The majority of the keywords land in counts that are below 200,000.  Outliers are two, mirroring the histogram. 
</pre>
[What does the histogram tell you about the data?  What does the boxplot tell you about the data?]

<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">

### (10 points) Trends in Relevant Subset 
```{r, chunk-plotting-jobs-trends-intro, cache.rebuild=TRUE}

jobs.subset$year.week = as.numeric( gsub("-",".",jobs.subset$year.week, fixed=TRUE) );


jobs.subset = sortDataFrameByNumericColumns(jobs.subset, c("year.week","job.count"), c("ASC","DESC") );
# easier to manage as "how many thousand jobs"
jobs.subset$job.count.k = jobs.subset$job.count / 1000;

do.nothing = plotJobs(jobs.subset);

do.nothing = plotJobs(jobs.subset, myy.lim = c(0,42) );

```
<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### Initial Perspective

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
The top two keywords (that we saw in histograms and boxplots as outliers) are "Microsoft Office" and "C++".  

I think the lines are parallelish because the data is tracking the last five weeks and there is a relative constant rate of change keeping the line straight-ish.  Interest in the keywords remains steady over the last five weeks. 

Other than "Git".  Can data entry really be considered a keyword for data analysis...?  Both Microsoft Office and data analysis are general keywords that can span an extremely wide range of occupations that I think these are nto necessarily the best keywords to use in a data analyst specific job search.  I would say that SQL would be the first keyword I would deem specific to data analysis, and therefore the most sought after skill for a majority of data anaylsis jobs.  

"Data analysis" seems to be trending a little higher than other keywords.  Also, "data analytics".  They seem to be increasing more than "C++", "Computer Science", "Python", etc.  

My first perspective?  Vs initial perspective? Not sure what the difference is, unless asking about my perspective on the most popular keywords prior to looking at any sort of data.  Looking back in my physical notebook, I see that I already looked at the data first, but I do know that I initially thought "python", "sql", "data analyst", "business analyst", and "R" would be keywords relevant to me in my future job search.  (These were keywords I put down as 5 terms that most interest me as a potential first "after college job" in the reflection for day 2 lecture video.)  I still stand by those as being the keywords most relevant to me, and glad to see that SQL and Python are some of the top keywords, with data analytics/analysis/analyst closely following SQL and Python.  
</pre>
[What is your initial "perspective" of this data, now that you see it?  Why are the lines "parallel-ish"?  What kind of a trend is that?

Now comment on the line of data for "Data analysis".  How is it trending?  How does it compare to other Search-Query Words?

What is your first perspective?

]

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### Missing Data "Git" Week 40?


```{r, chunk-plotting-jobs-trends-github-problem, cache.rebuild=TRUE}

do.nothing = plotJobs(jobs.subset, myy.lim = c(0,20) );

```


**Git-40/Git-41 data history, notice the date-time and file sizes ...** 

<IMG src="http://md5.mshaffer.com/WSU_STATS419/_images_/Git-40.png" style="border: 2px black solid;" />
<div>**Source: Data provenance history**</div>

<IMG src="http://md5.mshaffer.com/WSU_STATS419/_images_/Git-41.png" style="border: 2px black solid;" />
<div>**Source: Data provenance history**</div>

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
I don't know what is going on here, but it doesn't make sense that "git" dropped to zero from 18617.  Maybe it was a glitch either Indeed.com website for the week -- because it doesn't makes sense, as long as the scraping code wasn't changed, for it to not work on week 40, but then scrape properly the following week. 

No idea how it relates to the other data.  I Youtubed a few continuity and discontinuity calculus videos, but still don't know how to fit it into this situation.  I would probably remove it, and use NA. Or average Week 39 and Week 41 values.
</pre>

[Is this data missing or did it just drop to zero that week?  How does this relate to the other data (remember the idea of "continuity" in mathematics)?  What should you do about it?]

```{r, chunk-plotting-jobs-trends-github-solution, cache.rebuild=TRUE}

idxs.week.40 = which(jobs.subset$year.week == 2020.40);
idxs.Git     = which(jobs.subset$search.query == "Git");

# set notation
my.idx = intersect(idxs.Git,idxs.week.40);

jobs.subset[idxs.Git,];
jobs.subset[my.idx,];


## change this if you feel appropriate?  To what number? 
jobs.subset[my.idx,3] = 0;         # job.count
jobs.subset[my.idx,3] = 0/1000;    # job.count.k (in thousands) ...

do.nothing = plotJobs(jobs.subset, myy.lim = c(0,20) );

```
<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### Is "Microsoft Office" bigger than "C++"?

I use the term "bigger" or "better" intentionally.  We are comparing two items, and these are generic ways of communicating such a comparison.  In context of this data problem, a formalized form of the question would be something like:  "Utilizing job count for a given search query, determine if the query 'Microsoft Office' has a larger job count than the query 'C++'?"  This question will need to be formalized if we are trying to draw specific conclusions, but the initiation of analysis "which is bigger" allows us to understand what the data says or does not say, through exploration.  

To answer the question:

Mathematically, if two lines are parallel, and one is above the other, can we use **distance** to draw a conclusion?  Now, many times in statistics we deal with noise in the data, it is not "deterministic" but "stochastic" ... so we need to understand the variability.  Based on the data we see, can we not use  "parallel-line" logic to conclude that they are different?  This is one dimension of EDA, use mathematics. ("mathematics")

```{r, chunk-plotting-jobs-trends-microsoft, cache.rebuild=TRUE}

do.nothing = plotJobs(jobs.subset, myy.lim = c(80,250) );

boxplotJobQueryComparison(jobs.subset, "Microsoft Office", "C++");

```
Tukey invented the boxplot as a nice EDA representation of the data.  What logical inference can we make about the distances between the boxplots and the fact that no data is overlapping.  This is another dimension of EDA, use "distance" and the boxplot "IQR" to compare two elements.  What conclusion would we make? ("boxplot")

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
There is a differece between "Microsoft Office" and "C++". Both "mathematically" by measuring the count for each week and subtracting them to get the difference (or looking at the parallel-ness plus how wide the two lines are from each other) and using "boxplot".  Boxplot indicates that the minimum job count for "Microsoft Office" is above 220, and the maximum job count for "C++" is around 170.  The "Microsoft Office" boxplot is more interesting than "C++" boxplot which looks pretty standard with the median in the middle of the IQR box.  The "Microsoft Office" boxplot median is at the top of the the IQR box, suggesting a skew.  Also, there isn't a whisker or a maximum line, just a single outlier.

I think "formal tests" were derived from "mathematics"...since statistical analysis all has a basis in math...?

</pre>

[Can we conclude the data are different based on "mathematics" or "boxplot"?

Would a formal "inferential statistical test" tell us somethin g different than logical inference?  

How do you think "formal tests" were derived if not from "mathematics" and "boxplot" (EDA)?]

```{r, chunk-plotting-jobs-trends-microsoft-answer, cache.rebuild=TRUE}

# courage in trusting your intuition may require a fall-back ... for those that need it ...

t.test.jobs(jobs.subset, "Microsoft Office", "C++"); 

```

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### Is "Statistics" bigger than "Java"?

```{r, chunk-plotting-jobs-trends-statistics, cache.rebuild=TRUE}

do.nothing = plotJobs(jobs.subset, myy.lim = c(49,53) );

do.nothing = boxplotJobQueryComparison(jobs.subset, "Statistics", "Java");

#t-test for statistics vs java
t.test.jobs(jobs.subset, "Statistics", "Java"); 


```

For this data, I can descriptively report that the third-quartile `Q3` of "Statistics" is about equal to the `median` of "Java".  The inter-quartile range (`IQR`) of each overlap.  The minimum value of "Java" is larger than the minimum value of "Statistics".  The maximum value of "Java" is slightly smaller than the maximum value of "Statistics".

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
I ran a t-test on "Statistics" and "Java", and the large t-test p-value indicates that there is no difference between the two keywords (as indicated in the boxplot).  In this case, mathetmatically, the lines are not parallel to each other (indicating distance/difference).  They intersect at week 42! But, for this data, both boxplot and t-test indicate no difference.   
</pre>

[Use "mathematics" and "boxplot" and "ttest" to answer the question: Is "Statistics" bigger than "Java"?]

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### What about "Data science" and "Big data"?

```{r, chunk-plotting-jobs-trends-big-data, cache.rebuild=TRUE}

do.nothing = plotJobs(jobs.subset, myy.lim = c(14,17) );

boxplotJobQueryComparison(jobs.subset, "Data science", "Big data");

#t.test
#t.test.jobs(jobs.subset, "Big Data", "Data Science");



```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
This data is unclear to me at a glance.  Mathematically, "Big data" and "Data science" start off parallel, but then intersect with "Big data" sloping down and "Data Science" sloping faster than previous weeks.  Boxplot tells me that the spread of "Data Science" is wider then "Big Data", with a more "normal" distribution than "Big Data" where the median is skewed in the upper range of the IQR box.  The t.test function didn't work with this, even though I'm not entirely sure why.  However, depending on the t.test p-value, I would be better able to see if there is a difference between the two sets or not, as I feel that mathematically and boxplot wise, there is a difference, but since their parallel lines are relatively close together plus they intersect, there is more "cloudiness" in my reading of the visualizations.
</pre>

[Use "mathematics" and "boxplot" and "ttest" to make a conclusion.  

Next, think carefully about the nature of the data.  Is the "collection-approach" flawed to make a conclusion comparing job-counts of these specific keywords? How likely is it that a single-job posting may have both keywords?

This is an example where "data-integrity" knowledge would surpass the other three logical conclusions.

This intuition requires an understanding of what mathematicians call "set theory".  If I am doing an independent search on keywords, is it possible that one job would show up in multiple searches.  That is, being counted twice or more. 

Intuition and logic would also allow us to conclude that our other comparisons are "very likely okay".  Why?

What would be an improved approach to collecting the data that would allow me to more accurately compare these two keywords?

This is representative of why exploratory data analysis is essential.  It provides us insight into the domain and highlights the need for better data, if we can find it.

]

### Conclusions on logical inference

Distance is a fundamental unit of comparison.  We can use our "mathematical" understanding of distance.  We can use an "EDA" understanding of the data (e.g., the boxplot).  We need to understand the data sourcing and how that will relate to the logical conclusions we are trying to draw. 

When we transition to "confirmatory inferential statistics", we cannot leave our understanding of "maths" and "EDA" behind.  They are the foundation from which "inferential statistics" is built.  They are "logical inference".

<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">

## COMPUTING DISTANCES

If you recall, we had a notebook on collecting data from Wikipedia.  We documented the "data-provenance" protocols to make this happen.  We have documented and can replicate our data-collection strategies.

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

### (10 points) Data Provenance defined

Imagine you are preparing for a job interview.  Write a 90-second blurb describing "what is data provenance" and "why it matters".  I would suggest the STAR(S) approach mentioned in one of the notebooks.  Reference the "Wikipedia" project as an example of how one can implement the features.

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
High level, data provenance is about quality data.  It matters because poor quality data will result in poor quality information.  In data analytics, data provenance has specific  attributes that can be applied to ensure quality and results.   Two processes are iteratively used during data analytics -- version control to ensure reproducibility, and quality control to ensure quality information.

In my Intro to Multivariate Statistics class, one of the assignments was to collect climate data on Whitefish, Montana.  The goal is to have one great visualization for anyone, technical or not, to easily be able to understand Whitefish's climate.  

I started with data collection using Wikipedia's data on the city's climate.  I saved the original raw data using html and json to make sure I had reproducibility.  Then converted it into other forms for me to clean and manipulate.  During the data analysis process, I used Github to store my RMarkdown file which allows for version control during this phase of data analytics.  

Part of data provenance is to ensure your data cleaning is being done correctly.  "Data paranoia" is important at this step where I am double-checking to my data is being cleaned correctly or if I am missing chunks, or including unnecessary information.  

The result is one great visualization that summarizes all my work into one concise "picture" and ensure that the graphic is not misleading as well -- the final step in data provenance in data analytics.   



</pre>
[What is data provenance?

Probably about 200-250 words (with the 90 second limit)
]

### Geospatial distances

"Geo-spatial" studies are becoming much more common in the "data analytics" community, so let's use basic "latitude/longitude" data to formally talk about "distance."

So we will look at the 50 state capitals of America (USA).  Before that, let's examine some basic principles of distances using my hometown.

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

### (5 points) Distance from one input to multiple outputs

#### My Hometown "Columbia Falls, Montana" `cfalls`

- Find all ZIP codes within 22 miles of Columbia Falls, MT `cfalls` (use lat/long provide from the Wikipedia lookup)... build the bounding "box" and perform the post-hoc "radial distance" computations (as we did in the homework).

```{r, chunk-distances-compute-cfalls, cache.rebuild=TRUE}

# copy/paste __student_access__/_SECRET_/_SECRET_database_.txt into console...  or this won't work

cfalls.latitude = 48.37028; 
cfalls.longitude = -114.18889;
my.radius = 22; my.units = "mi"; #miles

# THIS is where these exam functions live ...
# source_url( paste0(path.github,"misc/functions-midterm-F2000.R") );  # should be 2020 ... oh well




cfalls.info = getNeighborsFromLatLong(22, 48.37028, -114.18889, "mi");

cfalls.info$neighbors;


hometown.info = getNeighborsFromLatLong(13, 37.3382, -121.8863, "mi");



############## plotting ##############
brown = "#ffe4c4";
green = "#014421";

my.state = "montana";
my.state.color = "#ffe4c4";

my.county = "flathead";
my.county.color = "#014421"; 

my.nearby.states = c("idaho", "washington", "oregon");


plotNeighbors(cfalls.info, 
                    state          = my.state, 
                    state.color    = my.state.color,
                    state.border   = 0.05,
                    county         = my.county, 
                    county.border   = 0.05,  # if you don't see the box, increase this to like 0.75
                    county.color   = my.county.color, 
                    nearby.states  = my.nearby.states); 

```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
I am not sure why the box is a rectangle, but after looking at the buildBoundingBoxFromRadiusAndGivenLatitudeLogitude function, I think there is something wrong with the delta.longitude equation.  A search on the internet provided this link: https://gis.stackexchange.com/questions/142326/calculating-longitude-length-in-miles, that says that the longitude is dependent on latitude, because degrees in longitude are dependent on its distance to the equator (or poles depending on how you look at it).

As for the visualizations, for all three, a few labels would have helped.  I looked up Montana etc. to figure out what the green represented in the second and third visualization.  (I guess I could have looked at the code as well....but a brief glance didn't tell me that info, though now that I'm still working on this question, I caught that in the code.  Like much of this degree, I feel that I never "get it" in the first pass-through, I require many pass-throughs for me to understand these assignments whether for "programming" or "statistics" classes :P)  I did like the box and the dots in the third visualzation. Pretty easy to read and guess that the bigger circle represented Columbia Falls, with the smaller dots as locations.  

So much more work (though I wonder if Tableau would be able to do it "quicker"), but like how the green represents a county, I think if the white dots were in the shape of the zip code, it would be "cooler".  Also, what are the black dots?  Going to look at the code some more...OK -- the black dots should not be black because I didn't see them as they were in the black box border for awhile.  They are easy to miss. Also, the code for "plotNeighhbors" says that the city should be a star, but it doesn't look like one to me, and star shape is good.
</pre>
[
- why is the box not a square, but a rectangle? ... see `factor.lat` and `factor.long` in function `buildBoundingBoxFromRadiusAndGivenLatitudeLongitude`
- critique the visualization ... what do you like? what would make it better?
]

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### Your Hometown of something like it
Instead of `cfalls.info`, you do `hometown.info` 

- a location in the continental US of your choosing (not in Montana, Alaska, or Hawaii). [Graphing will not work for Alaska/Hawaii, Alaska has "boroughs" not counties.]
- find the latitude/longitude of the location you have selected (how and where to look that up?)
- Initially start with a radius of `13 miles`
- When you run the code, note how many total "neighbors"; if it is less than 20; increase the "miles" so at least 20 results are returned.
- In the end, you should select a location and radius that works for you.  And its visualization also works.
- Be certain to review and update the parameters before calling these functions.


<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">

hometown.latitude = 00.00000; 
hometown.longitude = -000.00000;
my.radius = 13; my.units = "mi"; #miles
hometown.info = getNeighborsFromLatLong( ???? );
plotNeighbors(hometown.info, ????);  # you are going to have to change some of these parameters ... 
<pre>

```{r, chunk-distances-compute-hometown, cache.rebuild=TRUE}

hometown.latitude = 37.33821; 
hometown.longitude = -121.88633;
my.radius = 8; my.units = "mi"; #miles

hometown.info = getNeighborsFromLatLong(8, 37.3382, -121.8863, "mi");
hometown.info$neighbors;

hometown.nearby.states = c("Oregon", "Arizona", "Nevada")

plotNeighbors(hometown.info,
              state = "California",
              state.color = "#ffe4c4",
              state.border = 0.05,
              county = "Santa Clara",
              county.border = 0.05,
              county.color = "#014421",
              nearby.states = hometown.nearby.states)  

# you are going to have to change some of these parameters ... 

```

<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">


### U.S. State Capitals (cities)

From Wikipedia, we grabbed one page that listed the 50 U.S. cities that are designated the capitals of each individual state in America (United States of America).

Using the power of a `for-loop` we make our functions work for us.  We now have the data ready to go.

```{r, chunk-distances-load-data, cache.rebuild=TRUE}

capitals = utils::read.csv( paste0(path.mshaffer, "_data_/state-capitals/final/state-capitals.txt"), header=TRUE, quote="", sep="|");

colnames(capitals) = c("state", "capital", "latitude", "longitude", "capital.since", "area.sq.miles", "population.2019.est", "population.2019.est.MSA", "population.2019.est.CSA", "city.rank.in.state", "url");

# hack-add from https://en.wikipedia.org/wiki/ISO_3166-2:US
# TODO, grab this table "appropriately" as a new function
# Is there a dictionary for shortened city names?
# the long-field names is also an issue that needs to be improved upon in the next iteration.

capitals$st = c("AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD","MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC","SD","TN","TX","UT","VT","VA","WA","WV","WI","WY"); # ,"DC","AS","GU","MP","PR","UM","VI");

myLabels = paste0(capitals$capital, ", ", capitals$st);

capitals;
```

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### Initial Plotting

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

- Plot the data on `usmap` (ggplot2)

```{r, chunk-distances-plot_usmap, cache.rebuild=TRUE}

latlong = removeAllColumnsBut(capitals,c( "state", "st", "capital", "latitude", "longitude", "population.2019.est") );

# first two elements have to be this
latlong = moveColumnsInDataFrame(latlong, c("longitude","latitude"), "before", "state");

# for transform to work
library(usmap);    
latlong.transform = usmap_transform(latlong);
library(ggplot2);

### plot_usmap ...  

plot_usmap(fill = "#53565A", alpha = 0.25) +
  ggrepel::geom_label_repel(data = latlong.transform,
             aes(x = longitude.1, y = latitude.1, label = capital),
             size = 3, alpha = 0.8,
             label.r = unit(0.5, "lines"), label.size = 0.5,
             segment.color = "#981E32", segment.size = 1,
             seed = 1002) +
  scale_size_continuous(range = c(1, 16),
                        label = scales::comma) +
  labs(title = "U.S. State Capitals",
       subtitle = "Source: Wikipedia (October 2020)") +
  theme(legend.position = "right")


```

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

- Plot the data using maths `voronoi` (tripack)

```{r, chunk-distances-map, cache.rebuild=TRUE}


colors = rainbow(50, s = 0.6, v = 0.75);

## initial visualization ...
library(tripack);
# plot( voronoi.mosaic(latlong[,4:3], duplicate="remove"), col=colors, xlab="");
plot( voronoi.mosaic(x = latlong$longitude, y = latlong$latitude), col=colors, xlab="");
text(x = latlong$longitude, y = latlong$latitude, labels = latlong$capital, col=colors, cex=0.5);


```

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

- Plot the data on `map` (base)

```{r, chunk-distances-map-base, cache.rebuild=TRUE}

### how is any of the other visualizations really any better than a simple map ... with actual locations for Alaska/Hawaii?
library(maps); 
map('state', plot = TRUE, fill = FALSE, 
    col = "blue", myborder = 0.5
    );
points(x = latlong$longitude, y = latlong$latitude, 
                  col = "red", pch = "*", cex = 1);


```

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### (5 points) Comparing "Visualization Options"

Above, the data was displayed using three different visualization packages.

The first `plot_usmap` uses the ``ggplot2` methodology which is tied to the "tidyverse" landscape of the `R` community.

The second `voronoi.mosaic` uses the graph-theory topology known as Voroni partitioning <https://en.wikipedia.org/wiki/Voronoi_diagram> with the "base" plot function to visualize the topology.

The last one `map` is from the "base" environment.

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
Visually, I think the Voronoi Diagram is the prettiest.  Prettiest colors, and the lines created by it are also visually appealing. It looks almost artistic (which since actual mosaics and art can be used using the voronoi diagram principles, makes sense)

Functionally, I think that both the first and third visualization conveys information more accurately. The first one is accurate to capital location to state (somewhat, as it misses a bit because the state label bubble is so large), and it conveys capital names which could be important to sometone wanting to know the names of the capital.  
The third visualization is also more effective because it better conveys the location of the capitals within the states, as well as the location of Alaska and Honolulu in relation to where the continental US is -- which is useful because I feel that most Americans don't know how far away Hawaii actually is, including me for many years. However, to increase effectiveness in state location, we lose the state name data.

For me, the key factor that determines appropriateness is context.  Evaluating context is the difference between computers and humans.  That determines whether location accuracy or state name data is more important.  

</pre>
[
Visually, which is the most appealing to you?  Why?

Functionally, which presents the data most effectively? Why?

When we create visualizations, it is essential to portray the data accurately.  For example, there are times when putting Alaska/Hawaii next to California might be appropriate, and other times it might not be.

What is a one key factor that would determine this appropriateness?

]




<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### (5 points) Building the distance matrix

The dataframe we are using has been named `latlong` to represent the latitudes and longitudes of the 50 U.S. cities in America.


```{r, chunk-distances-setup, cache.rebuild=TRUE}

# manual conversion
# how many miles is 1 degree of latitude
latitude.factor = 69;  # rough mile estimate  # 68.703 ?
latlong$x.lat = latlong$latitude * latitude.factor;

longitude.factor = 54.6;  # rough mile estimate  
latlong$y.long = latlong$longitude * longitude.factor;

latlong = moveColumnsInDataFrame(latlong, c("y.long","x.lat"), "before", "longitude");
```

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

Let's start with geo-spatial distances.  I will do `distMeeus` and you will do `distHaversine`

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

##### **Meeus**
These distance formulas can utilize the true geo-spatial coordinates.  The distance table is getting large, so there is a helper function to lookup a certain value.

```{r, chunk-distances-meeus, cache.rebuild=TRUE}

library(geosphere);
library(measurements);


dist.meeus = conv_unit(  distm( latlong[,3:4],
                  fun=distMeeus),  "m", "mi");  # default meters to miles

dist.meeus.m = as.matrix( dist.meeus );
  rownames(dist.meeus.m) = 
  colnames(dist.meeus.m) = myLabels;

dist.meeus.df = as.data.frame( round( dist.meeus.m, digits=1) );

dist.meeus.df;  ## too big

lookupPairwiseValue(dist.meeus.df, "Juneau, AK", "Montgomery, AL");

```
<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

##### **Haversine**
```{r, chunk-distances-haversine, cache.rebuild=TRUE}

dist.haversine = conv_unit(  distm( latlong[,3:4],
                  fun=distHaversine),  "m", "mi");  # default meters to miles

dist.haversine.m = as.matrix( dist.haversine );
  rownames(dist.haversine.m) = 
  colnames(dist.haversine.m) = myLabels;

dist.haversine.df = as.data.frame( round( dist.haversine.m, digits=1) );

dist.haversine.df;  ## too big

lookupPairwiseValue(dist.haversine.df, "Juneau, AK", "Montgomery, AL");


```


You can compare the two with the code below (currently in comments).

```{r, chunk-distances-haversine-comparison, cache.rebuild=TRUE}
 
x = dist.meeus.df[,2]; # Juneau ... pick one location
y = dist.haversine.df[,2];

my.lim = c(0, 4000 );

plot(x, y,  
             xlab="Meeus", 
             ylab="Haversine", 
             main="dist( Juneau, AK )",
             xlim=my.lim, 
             ylim=my.lim);
 
plotXYwithBoxPlots(x, y,  
             xlab="Meeus", 
             ylab="Haversine", 
             main="dist( Juneau, AK )",
             xlim=my.lim, 
             ylim=my.lim);

#?distMeeus
#?distHaversine
#lm.fit = lm(y~x)
#plot(lm.fit)
#summary(lm.fit)
```


<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
Differences between the two: I don't see too much difference between the two. The plot shows a nice, positive straight line, and while there are slight differences (after looking through a few of the rows in the two datasets to compare), looking at the visualizations makes me feel like there is no real difference between the two.  Results are similar.

Conceptual difference between Meeus and Haversine: Meeus uses WGS84 global ellipsoid measurements to calculate distance vs Haversine that uses a spherical earth  measurements without accounting for the ellipoid shape of Earth.  
Meeus is accurate re: the global ellipsoid WGS84, but for some applications using a printed map, other ellipsoids are preferable.

Better Algorithm? distGeo should be more accurate according to R help, but I had some trouble finding out what the difference between distGeo is to Meeus (other than distGeo is using Karney's method)
</pre>
[
Examining the data above, is there much difference between these two calculations? 

What is the conceptual difference between these two calculations?  Try `?distMeeus` or `?distHaversine`

Are the results similar?

Is there a more accurate distance algorithm for geo-spatial calculations?  If so, what is it?

]

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

Let's now do `manhattan` and `euclidean` which are more common in the "statistical clustering" domain.  I will do `euclidean`.


<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

##### **Manhattan**
```{r, chunk-distances-manhattan, cache.rebuild=TRUE}
dist.manhattan = dist( latlong[,1:2],
                      method="manhattan", diag=TRUE, upper=TRUE);
dist.manhattan.m = as.matrix( dist.manhattan );
  rownames(dist.manhattan.m) = 
  colnames(dist.manhattan.m) = myLabels;

dist.manhattan.df = as.data.frame( round( dist.manhattan.m, digits=1) );

dist.manhattan.df;  ## too big

lookupPairwiseValue(dist.manhattan.df, "Juneau, AK", "Montgomery, AL");

```

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

##### **Euclidean**
We have converted the true latitude/longitude to a miles-type format, so the resulting table will report miles.
```{r, chunk-distances-euclidean, cache.rebuild=TRUE}

dist.euclidean = dist( latlong[,1:2],
                      method="euclidean", diag=TRUE, upper=TRUE);
dist.euclidean.m = as.matrix( dist.euclidean );
  rownames(dist.euclidean.m) = 
  colnames(dist.euclidean.m) = myLabels;

dist.euclidean.df = as.data.frame( round( dist.euclidean.m, digits=1) );

dist.euclidean.df;  ## too big

lookupPairwiseValue(dist.euclidean.df, "Juneau, AK", "Montgomery, AL");


```
You can compare the two with the code below (currently in comments).


```{r, chunk-distances-euclidean-comparison, cache.rebuild=TRUE}
 
x = dist.manhattan.df[,2]; # Juneau ... pick one location
y = dist.euclidean.df[,2];
 
my.lim = c(0, 4000 );
 
plot(x, y,
             xlab="Manhattan",
             ylab="Euclidean",
             main="dist( Juneau, AK )",
             xlim=my.lim,
             ylim=my.lim);
 
plotXYwithBoxPlots(x, y,
             xlab="Manhattan",
             ylab="Euclidean",
             main="dist( Juneau, AK )",
             xlim=my.lim,
             ylim=my.lim);

```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
There is a difference between these two calculations, certainly more than between Meeus and Haversine. You can see it in the scatterplot.  While it still follows a positive, diagonal line, without any outliers from the diagonal line, it's not as tight of a line as the plot of Meeus/Haversine.

but I still think results are similar.
</pre>
[
What is the difference between these two calculations? 

Are the results similar?

]


<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">

## HIERARCHICAL clustering as a function of distance

Aggregating or agglomerating data is typically called "clustering" and is generally considered to be a "unsupervised learning" method.


### Introduction
We use `hclust` to perform Hierarchical Clustering.  The word "hierarchical" is used because the nature of the data is organized like a family genealogy.  The bottom of the tree represents the descendants that eventually "link" to a common ancestor.

If you type `?hclust` you can review the parameter options which we explored in a notebook.  `hclust` is primarily a function of `dist()` [distance], and we have just computed some distances, so we could try and apply `hclust` to our data to see if our state capitals will cluster into meaningful regions.

There are several agglomeration methods ("linkage") one could choose from.  Each method takes the `dist` <https://en.wikipedia.org/wiki/Hierarchical_clustering#Metric> and performs some pairwise distance algorithm called a linkage criteria <https://en.wikipedia.org/wiki/Hierarchical_clustering#Linkage_criteria>.

I generally use either the "complete" linkage method or the "ward.D2" linkage method.  You can read help `?hclust` to better appreciate why: "A number of different clustering methods are provided. Ward's minimum variance method aims at finding compact, spherical clusters. The complete linkage method finds similar clusters."

If you are trying to link binary data (zeroes and ones) or genomic data (where the distances were computed using a genetic-distance algorithm), you may want to try the UPGMA approach: "average" or "centroid".  

### Analogy of Family

Since I am from a large family, I will make a family tree analogy.  For me, I am the 5th of 11 siblings (5 brothers, 5 sisters).  If I wanted to cluster the siblings in a pair-wise fashion, how would I begin?

First, I would ask, which other sibling is most like me?  Since this is a pair-wise approach, and there are 11 siblings, maybe at the initial stage, I will not be paired with another sibling.

In fact, at least one will not be paired because the total number is 11.  And maybe more than one will not be paired in the initial stage.  Remember "similarity" is being defined based on some distance-linkage method.  And to use this approach, "similarity" needs to numerical data.

At each stage, pair-wise joining occurs until there is nothing left to join.  The tree contains all of the elements.  Every element (branch) eventually joins the main branch (trunk) of the tree.  

### Clustering U.S. capital cities based on latitude, longitude

We already have some data for the U.S. capitals and have computed a Euclidean distance using a capital-city's longitude and latitude.  Let's choose to cut the result into `12` agglomerations.  Why 12?  It was a choice based on my life experience and intuition.  As I reflected on why, I did some external searching that validates a choice in that range <https://en.wikipedia.org/wiki/List_of_regions_of_the_United_States>.  You certainly could run this analysis with another choice  It is exploratory, and your intuition matters.

Geographically, I am saying the capital-city does represent the state.  An ideal representation may be in the center (centroid) of the state.  Remember this when we see the linkages with "New York", the city is Albany, not Manhattan.

```{r, chunk-distances-hclust-euclidean, cache.rebuild=TRUE}

## ward.D2
hclust.ward2.dist.euclidean = hclust(dist.euclidean, method="ward.D2");

plot( hclust.ward2.dist.euclidean, 
      labels= myLabels );
rect.hclust( hclust.ward2.dist.euclidean, k=12 );

## complete
# 
hclust.complete.dist.euclidean = hclust(dist.euclidean, method="complete");
# 
plot( hclust.complete.dist.euclidean, 
       labels= myLabels );
rect.hclust( hclust.complete.dist.euclidean, k=12 );
#   

```

### Understanding the `cutree`

In the above example, the tree was cut into 12 groups based on the "distance" formula used and based on the "agglomeration" linkage technique invoice.  I likely should have used a "geo-spatial" distance, but we will see that mere "euclidean" distance seems to perform okay.  

A fancy word for this statistical tree is a "dendrogram".  I call each element of the tree a branch.  The smallest branches (twigs) are the fundamental elements, in this case the cities.  Over time, they merge with other small branches, and so on.

The relative height when this smallest branch merges with another branch demonstrates when the branch has found a similar pair-wise match (with another smallest branch or a merging branch).  I call "Honolulu Hawaii" an isolate because the vertical height when it merges with another branch is the highest of all of the smallest branches (e.g., cities).  "Juneau Alaska" is another isolate, but it does merge before "Hawaii".

It would be nice if we could decompose this information and look at one `cutree` at a time.  And color-code the distinctions.  Using the function `plot.hclust.sub` we can do this.

```{r, chunk-distances-hclust-euclidean-sub, cache.rebuild=TRUE}

source_url( paste0(path.github,"humanVerseWSU/R/functions-EDA.R") );  # EDA functions ...


hclust.ward2.dist.euclidean$labels = myLabels;
plot.hclust.sub(hclust.ward2.dist.euclidean, k=12);

plot.hclust.sub(hclust.complete.dist.euclidean, k=12);
```

### (10 points) Review one clustering tree (dendrogram)

Choose either `hclust.ward2.dist.euclidean` or `hclust.complete.dist.euclidean` and review how the U.S. state capitals are clustered.  [I commented out one form, so you will have to re-run if you want to select that one.]

Comment on the "face validity" of this approach based on your understanding about how the U.S. regions are defined?  Are the North/South Dakotas together?  What about the North/South Carolinas?  What about the Pacific Northwest?  While living in Kentucky, some people called the area "Kentuckiana" meaning Kentucky/Indiana.  Does that show up?  Also note anything that seems peculiar.

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
I used hclust.ward2.dist.euclidean because the subtrees were labelled names as opposed to numbers (I choose the easiest/lazy option).  

I think that hierarchial clustering based on distance is a good way to divide the United States without further research (basing my knowledge of US regions from memory--which, sadly, I have a terrible memory). North and South Dakota are clustered together. It did cluster Kentucky and Indiana as the closest subtree. 

In my head these are the regions of the US: Pacific Northwest, Southwest, Texas, South, Northeastern States, East Coast, Dust Bowl, West, and maybe Florida, plus Alaska, and Hawaii.  Both Florida and Texas are weird standalone states that should be lumped into "South", but I don't. Of course, these are from my memory, and I admit I am weakest in my knowledge of most of the middle of the US in between the two coasts and their immediate neighboring states.

In the South: North and South Carolinas are clustered together, but the closest subtree is NC and VA together, not NC and SC -- which does make sense. In my head, North Carolina is not a part of the South. I grew up outside of Philadelphia, and I consider North Carolina and Virginia to be borderline Northern States, but South Carolina is firmly "Southern". When my husband (I apologize that I pepper my responses with him, but I am a pretty isolated stay-at-home mother and I definitely need to get out more (that will be when I graduate and get a job hopefully)!!) was considering looking for another company out of Tucson, I told him "anywhere except the Dust Bowl and the South".  He interviewed for a company in North Carolina -- which I never considered "South"! To me, the "South" is Alabama, Louisiana, Georgia, South Carolina, Louisiana, Missouri, and Arkansas, with Florida being another "borderline South" state like North Carolina. The tree did not ballpark the "South" very well.

In the Pacific Northwest: The tree pinpointed what I consider the Pacific Northwest, mostly accurately.  It did include Nevada (which makes sense in terms of distance but not in terms of traditional US regions).

In the Northeastern States: Subtree #5 did a good job of what I would consider "Northeastern States".

In the East Coast: Subtree #7 also did a good job of what I consider "East Coast States".  The only exception is New York, which is placed in the Northeastern States.  In my head, New York, NY is the "real" capital and it is closer to Philadelphia and New Jersey than Albany is.  

I can't really comment too much about the middle of America states and the tree's accuracy.  I've always lived on an outer edge of the US my whole life -- born in Texas, lived in Florida and Philadelphia, then Arizona, and now California. 

</pre>


### Additional remarks about `hclust`

I find `hclust` to be a nice initial perusal of the data.  

If you want to understand the stability of a particular `hclust` to use it for something other than an "initial perusal of the data," I would recommend `pvclust` which I introduced in the weekly notebooks.  It is often used in peer-reviewed research, as it provides a `p-value` of sorts regarding the **stability** of the `hclust` structure.



<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">

## GENERIC clustering 
Aggregating or agglomerating data is typically called "clustering".  In exploration, we can create some generic clustering techniques.  Below we will create two adhoc clustering rules.

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

### Arbitrary Aggregation

Recall the "movie dataset" with "Will Smith" and "Denzel Washington".  We have a collection of movies, and how much money each movie made at the box office.  We could organize those movies by some arbitrary rules.  For example:

- Cluster 1:  NA.  We have missing data regarding the money.  So let's put all movies that are NA into that cluster.
- Cluster 2:  Under a million dollars
- Cluster 3:  1-4.99... million dollars (greater than or equal to one, but less than 5)
- Cluster 4:  5-49.99... 
- Cluster 5:  50+

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

#### (5 points) Movie Aggregation [Arbitrary] for Will and Denzel
```{r, chunk-aggregate-arbitrary, cache.rebuild=TRUE}

library(devtools);
source_url( paste0(path.mshaffer, "will") );
source_url( paste0(path.mshaffer, "denzel") );

movies.50 = rbind(will$movies.50, denzel$movies.50);

unique(movies.50$ttid); # are they in any shared movies ???

loadInflationData();

movies.50 = standardizeDollarsInDataFrame(movies.50, 
                2000, 
                "millions", 
                "year", 
                "millionsAdj2000");

movies.50$cluster.arbitrary = NA;
str(movies.50);

## you do something here ... 

# (1) populate cluster.arbitrary
##need to find and replace according to cluster category

for (i in 1:length(movies.50$millionsAdj2000))
{
  if(is.na(movies.50$millionsAdj2000[i]))
  {
    movies.50$cluster.arbitrary[i] = 1
  }
  else if(movies.50$millionsAdj2000[i] < 1)
  {
    movies.50$cluster.arbitrary[i] = 2
  }
  else if(movies.50$millionsAdj2000[i] < 5)
  {
    movies.50$cluster.arbitrary[i] = 3
  }
  else if(movies.50$millionsAdj2000[i] < 50)
  {
    movies.50$cluster.arbitrary[i] = 4
  }
    else
  {
    movies.50$cluster.arbitrary[i] = 5
  }
}
# (2) summarize how many movies live in each (table count)
library(plyr)
cluster_count = count(movies.50, vars = "cluster.arbitrary" )

cluster_count
```

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

### Aggregating using Quantiles

John Tukey emphasized that ordering the data and then splitting it based on the ordering was a fundamental premise of exploratory data analysis.  

#### Tukey's Summary Data

John Tukey proposed five elements as primary data for analysis.

- the minimum `min`
- the maximum `max`

Sorting the data makes it easiest to find these data, and will be useful to find the other three exploratory summary features.

This is what I would call "slice and dice".  The data is cut in half, and the value of that middle "cutting point" is the `Q2` which we call the `median`.

Next, the lower half could also be cut in half, and the value of that middle "cutting" point is `Q1`.

Then, the upper half could also be cut in half, and the value of that middle "cutting" point is `Q3`.

A common metric derived from this "median-split" procedure is called the interquartile range `IQR` which is defined as the distance between `Q3` and `Q1`.  It literally represents the middle 50% of the data; 50% of the elements of the dataset are between `Q3` and `Q1`.  


<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">
#### Quartile Example 
```{r, chunk-quartiles, cache.rebuild=TRUE}

x = 1:99;
length(x);

median(x);
x[50];

median(x[1:49]);
x[25];

median(x[51:99]);
x[75];

# probability-approach ... 
# algorithm: the default type=7
stats::quantile(x, prob = c(0.25, 0.5, 0.75), type=1);

```

Algorithms address various issues associated with dividing numbers and whether or not to include the dividing number in the subset division, but the principle holds.

We can generalize this idea by not always cutting the data in half (median-split as `n=2`, median-median-split as `n=4`).  Instead, we could cut by tens (we call them deciles).  Or we could cut by hundreds (we call them centiles).  The function `quantile` performs this operation, and if you dig into the `doStatsSummary` function used in this course, you can see its application.


<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">
#### (5 points) Movie Aggregation [Decile] for Will and Denzel

```{r, chunk-aggregate-quantiles, cache.rebuild=TRUE}

movies.50$cluster.deciles = NA;
str(movies.50);

## you do something here ... 

# use # stats::quantile(x, prob=seq(0.1,0.9,by=0.1), type=1 );
# (1) how many NA's are there ... keep them NA's
# (2) for the rest of the data, break it up into deciles
# (3) $cluster.deciles for a given movie should be NA, 1, 2, 3, ... 10
# (4) summarize how many movies live in each (table count)

vect = quantile(movies.50$millionsAdj2000, prob = seq(.1,.9, by=.1), type=1, na.rm=TRUE)
vect
vect[1]

for (i in 1:length(movies.50$millionsAdj2000))
{
  if(is.na(movies.50$millionsAdj2000[i]))
  {
    movies.50$cluster.deciles[i] = NA
  }
  else if(movies.50$millionsAdj2000[i] < vect[1])
  {
    movies.50$cluster.deciles[i] = 1
  }
  else if(movies.50$millionsAdj2000[i] < vect[2])
  {
    movies.50$cluster.deciles[i] = 2
  }
  else if(movies.50$millionsAdj2000[i] < vect[3])
  {
    movies.50$cluster.deciles[i] = 3
  }
  else if(movies.50$millionsAdj2000[i] < vect[4])
  {
    movies.50$cluster.deciles[i] = 4
  }
  else if(movies.50$millionsAdj2000[i] < vect[5])
  {
    movies.50$cluster.deciles[i] = 5
  }
  else if(movies.50$millionsAdj2000[i] < vect[6])
  {
    movies.50$cluster.deciles[i] = 6
  }
  else if(movies.50$millionsAdj2000[i] < vect[7])
  {
    movies.50$cluster.deciles[i] = 7
  }
  else if(movies.50$millionsAdj2000[i] < vect[8])
  {
    movies.50$cluster.deciles[i] = 8
  }
  else if(movies.50$millionsAdj2000[i] < vect[9])
  {
    movies.50$cluster.deciles[i] = 9
  }
    else
  {
    movies.50$cluster.deciles[i] = 10
  }
}
# (2) summarize how many movies live in each (table count)
library(plyr)
cluster.deciles_count = count(movies.50, vars = "cluster.deciles" )

cluster.deciles_count

```

<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">

## CENTROID clustering (k-means) as a function of distance

### Introduction
Rather than clustering on distance-linkage in a pair-wise fashion, we can cluster based on randomly selecting just `k` points in our data and begin identifying their nearest neighbors using some distance approach.

For example, if `k=3`, we would randomly select three of our data points.  We would then compute the distances from all of the remaining points to these `3` anchor points.  The points that are closest to a given anchor will be assigned to that anchor.  At the end of the stage, we now have new data, so a new centroid is determined.  At this point, the centroid <https://en.wikipedia.org/wiki/Centroid> is likely not one of our data points, but a location within the given centroid cluster.  In the "naive" approach, you merely take an average (mean) of all the members of your cluster.  More advanced approaches (the default "Hartigan-Wong" of `kmeans`) utilize deviations from the average, called a sum-of-squares approach.  This is why in our `kmeans-notebook` we analyzed the `wss` to ascertain how many clusters `k` should we use.

Regardless, after the new centroids (centers) for the clusters are determined, the process iterates.  All distances are computed from all points to the new centroids; points are assigned to a given centroid cluster (in this example: 1, 2, 3); a new centroid center is computed, and we repeat the process until a stopping rule is reached: maybe we have exhausted the number of iterations allowed (`iter.max` parameter of `kmeans`)?  Or maybe we are not changing membership of any of the data points?  Or maybe we have met some objective (like `wss`)?

It is possible to get a `kmeans` result by merely starting with `3 different` random points.  In general, `kmeans` is fast (and our computers are so much faster than the computers of 1990: the first computer I built in 1995 had 32MB of RAM, a Pentium processor <https://en.wikipedia.org/wiki/Pentium#Pentium> and a 900MB Cheetah hard-drive).

Anyway, we can utilize the parameter `nstart` to try many different starting values, and let the program identify the best, most consistent solution.

### My recommendations

I would recommend the default algorithm `Hartigan-Wong` with `iter.max=100` and `nstart=100`.  You can test the timings from the default values: `iter.max=10` and `nstart=1`.  Since the data is likely multidimensional `stars` are the best way to summary the results and membership of `kmeans`.  Please see the "kmeans-notebook" for examples.

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

### WIKIPEDIA CLIMATE DATA

For the 50 U.S. cities, we harvested climate data.  In the "Wikipedia" notebook, details of that data provenance were outlined.  If we want to compare the cities using the climate data, we have to build a matching dataframe, which means we have to select features that exist in each city's climate data set.

There are four temperature features that are always present:

- Record high F (C) ... we will call it "high.max"
- Average high F (C) ... we will call it "high.avg"
- Average low F (C) ... we will call it "low.avg"
- Record low F (C)  ... we will call it "low.max" [probably not the best name choice, but it is parallel in form to the first element]

Additionally, for precipitation we have:

- Average precipitation inches (mm) ... we will call it "rain"
- Average snowfall inches (cm) ... we will call it "snow"

For each of these features we have 12 months of data.  This is a nice dataset.  Let's see what we can do with it.


#### Basic Background Research
We should begin by doing a bit of peripheral research on the topic, to gain "domain knowledge" so we know what to do with the data.  A few elements that would benefit.


<https://www.forbes.com/sites/brianbrettschneider/2018/07/08/when-does-the-hottest-day-of-the-year-usually-occur/#78141f47548c>

<IMG src="http://md5.mshaffer.com/WSU_STATS419/_images_/max-temp.jpg" style="border: 2px black solid;" />
<div>**Source: https://bit.ly/359HAnY**</div>

<https://www.climate.gov/news-features/featured-images/whats-coldest-day-year>


<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### One Graph

When graphing data to visualize, it is essential that you keep the scales uniform so quick-visual comparisons are accurate.  I gave you a task to practice the idea of creation that "one informative" research graphic.  And I now present my version for you to use and critique based on your efforts.  I am a `plot` guy, so some of you may have a different ``ggplot2` type solution.  Ultimately, the intent of this graphic is to best summarize the data in a meaninful way for exploratory analysis.

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">
##### (5 points) One Research Graph

```{r, chunk-hclust-climate-one-research-graph, cache.rebuild=TRUE}
climate = utils::read.csv( paste0(path.mshaffer, "_data_/state-capitals/final/state-capitals-climatedata.txt"), header=TRUE, quote="", sep="|");


plotTemperatureFromWikipediaData(climate, city.key="capital", city.val="Helena");

plotTemperatureFromWikipediaData(climate, city.key="capital", city.val="Baton Rouge");

```

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
I like that the graphic shows the flow of the weather in one image. I like that there is a shade between the max and min temperature ranges for the month.  The shading shows more clearly the range -- i.e. if the difference between max and min temperature is big, than the shaded region is big (such as February vs June/July). It seems taht summer temperatures are less variable than winter.  I like that the max temperature font is bigger so it clearly shows that it is the max temperature.  I like the different colors for hot vs cold weather. 

I think I would have used different graphics between snow and rain. Also, maybe a navy blue for the cold -- the black in Montana graph was harder for me to interpret at first glance -- we've been conditioned to associate red with hot and blue with cold. The sun graphic could be slightly smaller.

I think it is aesthetically pleasing because people like color changes -- so seeing the temperature lines being graded from red-yellow-blue-black is "fancy" and also aesthetically pleasing. 

It is functional.

</pre>
[
What do you like about this graphic?  

What do you dislike?

Is it aesthetically pleasing?
Is it functional?

]

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">
We can achieve a side-by-side comparison using the function described below.  The first city will be graphed on the left, the second city on the right. 

```{r, chunk-hclust-climate-one-research-graph-comparison, cache.rebuild=TRUE}

compareTwoCitiesClimates(climate, city.key="capital", city.1="Helena", city.2="Baton Rouge");


```



<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
I like that you don't need to scroll up and down to compare the two graphics. For some reason, in this smaller version, I am able to differentiate between snow and rainfall easier -- not sure why though. 

I think the sun graphic is too large for the smaller size of the graph.  The y axis is the same scale, but not the same tickmarks.  I think it would be better if the tickmarks/range was the same, and aligned with each other.  That would make it another visible way to differentiate the differences in temperature between Montana and Louisiana because then you could see that Louisiana ranges were higher than Montana ranges.  As it is right now, you can't differentiate that easily visibly (only visible method would be the different color -- red vs orange, blue vs black).  

The difference between snow and rain is that snow had blue shading above the blue circles.  I think I would have shaded it white (as people associate white with snow, blue with rain).  Or used snowflakes instead. 

</pre>
[
What do you like about this graphic?  

What do you dislike?

Are the y-axis the same scale?  Are the visible gridlines for each the same?

What is the difference between rain and snow on the graphic?  Was that a good approach?  How would you have done it?

]


<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

##### (5 points) One Publication Graph

We are in exploration, so the "one" research graphic may be very different than the "one" formal graphic designed for a client.  Typically, the final graphic needs to meet certain criteria:

- Very pleasing aesthetically 
- Interactive if possible
- Live data feeds if possible
- Served in a secure, safe, private location (if required by the client)

To demonstrate the differences, I created a mockup of our 50 U.S. cities and put it into a nice finalized product form called "highcharts".  

<http://md5.mshaffer.com/WSU_STATS419/_EXAMPLES_/fiddle_usmap/>

It pulls data in real time (using AJAX) to grab the weather at the latitudes/longitudes we defined in our Wikipedia notebook.  

- You can use your mouse to draw a box to zoom in.  
- The third-wheel on the mouse also helps you zoom in/out.
- Hold down CNTRL with your left hand and use your mouse key to drag the map.  It seems to only "pan" in the x-direction at the moment.  
- The data in the popup displayed can be customized.  I report the temperature in Celsius/Fahrenheit and also display the population data we gathered from Wikipedia. 

Once you have a template built, it is rather easy to modify it.  Here I changed the background map, and all of the data/features stay the same:

<http://md5.mshaffer.com/WSU_STATS419/_EXAMPLES_/fiddle_usmap/world.html>


<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">
### Which Features to Include in the Analysis

* months, you can pick all of them `1:12` or maybe just one month per season (April, July, October, January)
* X, depending on what you chose for months, you can now select what climate columns you want to use
  - Some of Temperature Data
  - All of Temperature Data
  - Precipitation Data
  - Everything (All of Temperature Data, Precipitation Data)
  
If we want to cluster cities, which decisions seem best?  Why?  As you can see from the code below, you just comment out two options, and can quickly rerun the analysis.

- WHICH MONTHS
- WHICH COLUMNS

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">
#### WHICH MONTHS & WHICH COLUMNS
```{r, chunk-climate-which-features, cache.rebuild=TRUE}

climate = utils::read.csv( paste0(path.mshaffer, "_data_/state-capitals/final/state-capitals-climatedata.txt"), header=TRUE, quote=
                             "", sep="|");

##################### WHICH MONTHS #####################
########################################################
months = 1:12; # all the data
#months = c(1,4,7,10); # one month of each of the four seasons
########################################################


month.abb;  # ?month.abb
month.name;  

month.name[months];  # these are the names of the months you are selecting ...


# this function would allow us to use different months as criteria and different climate-data keys.  It is variadic and flexible.  `key.n` are the names we will use for our new columns ...

climate.df = buildClimateDataFrame(climate, months, keys=c("Record high F (C)", "Average high F (C)", "Average low F (C)", "Record low F (C)", "Average precipitation inches (mm)", "Average snowfall inches (cm)"), keys.n = c("highmax", "highavg",  "lowavg", "lowmin", "rain", "snow") );

climate.df;
names(climate.df); # this helps you see the indexes ...

##################### WHICH COLUMNS #####################
########################################################
#X = climate.df[,5:52];  # temperature
X = climate.df[,5:76];  # everything (includes rain)
#X = climate.df[,5:20];  # temperature, 1 month per season
#X = climate.df[,5:28];  # everything (includes rain), 1 month per season
########################################################


  rownames(X) = climate.df$labels;
Xs = scale(X);
```

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">

### To scale or not to scale, that is the question

```{r, chunk-distances-X-or-Xs, cache.rebuild=TRUE}

X = climate.df[,5:76];  # everything ... you have to change months above to get this dataframe to be the correct size ... months = 1:12
  rownames(X) = climate.df$labels;
Xs = scale(X);

```


So let's do some analysis with all of the data available to us.  Most of the data is in Temperature, with ranges from -42 degrees Fahrenheit (Helena, Montana) to 122 (Phoenix, Arizona).

The precipitation data (rain and snow) is measured in inches.  So should we scale the data.  The answer in PCA and orthogonal projections is absolutely YES, but for `hclust` and `kmeans` is that always the case?

You can make a choice below, and observe how it influences your answers. 

<HR divider="1" style="border: 1px solid #981e32; border-radius: 1px;">
#### WHICH X
```{r, chunk-distances-whichX, cache.rebuild=TRUE}

whichX = X;
#whichX = Xs;

```

<HR divider="5" style="border: 5px solid #981e32; border-radius: 5px;">
### Perform `k-means` on All Climate Features

With the selected features let's perform k-means.  Let's select k=12.  I will select all of the features, but you can change that if you wish.

#### Descriptives of Sample
```{r, chunk-distances-kmeans-setup-temp, cache.rebuild=TRUE}



colors = rainbow(50, s = 0.6, v = 0.75); # 50 colors for 50 states

# descriptive star plot to start
stars(whichX, len = 0.5, key.loc=c(12,2), draw.segments = TRUE);

## too busy, let's group them
x.start = 1;
x.end = 10;
for(i in 1:5)
  {
  stars( whichX[x.start:x.end,] , 
          len = 0.5, key.loc=c(6,2), draw.segments = TRUE);
  x.start = 1 + x.end;
  x.end = 10 + x.end;
  }

```

Above, you are just analyzing the general shapes.  Which ones are "fuller" circles?  Why?  

Which ones are not very "full circles"?  Why?

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### Computation of Clusters/Centroids


```{r, chunk-distances-kmeans-run-temp, cache.rebuild=TRUE}

k = 12;
iterations = 100;
number.starts = 100;

whichX.kmeans = kmeans(whichX, 12, 
                    iter.max=iterations, 
                    nstart = number.starts);  # default algorithm
stars(whichX.kmeans$centers, len = 0.5, key.loc = c(10, 3),
        main = "Algorithm: DEFAULT [Hartigan-Wong] \n Stars of KMEANS=12", draw.segments = TRUE);

```

<HR divider="2" style="border: 2px solid #981e32; border-radius: 2px;">

#### Cluster Membership and Centroid Attributes


```{r, chunk-distances-kmeans-table-temp, cache.rebuild=TRUE}



membership = matrix( whichX.kmeans$cluster, ncol=1);
membership = membership[order(membership),];
membership = as.data.frame(membership);
        rownames(membership) = climate.df$labels;
        colnames(membership) = c("Cluster");

membership;

print( table(membership) ) ; 

# I believe in an older version of R these were called $centroids
attributes = as.data.frame( whichX.kmeans$centers );
    rownames(attributes) = paste0("Cluster.",1:12);
  attributes;
  
```

#### (10 points) Summarize Findings

- Identify which states share a common cluster. 
- For a given cluster, what are its primary characteristics 

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
Cluster shapes in the star plot seem to correspond to the extremity of temperature range.  I.e Phoenix, AZ is very hot -- and so has a fuller circle shape -- where it is not a circle shape and "blank" corresponds to their winter temperatures which are closer to 0 degrees.  I.e. I think that the ones with not "full circles" are cities that have temperature ranges closes to 0 degrees Fahrenheit.  Those with bigger circles indicate higher temperatures in those corresponding months.  

Note: I ran the cluster code twice, and I'm not sure why, but the data seems to cluster differently between the two times. I am entering cluster data for the one of these iterations. (K-means can generate random results)

Cluster 1: Montgomery, AL; Juneau, AK; Phoenix, AZ; Little Rock, AK

Cluster 2: Sacramento, CA; Denver, CO; Hartford, CT

Cluster3: Dover, DE; Tallahssee, FL; Atlanta, GA; Honolulu, HI; Boise, ID

Cluster 4: Springfield, IL; Indianapolis, IN; Des Moines, IA; Topeka, KS; Frankfort, KY; Baton Rouge, LA; Augusta, ME; Annapolis, MD; Boston, MA; Lansing, MI; Saint Paul, MN; Jackson, MS

Cluster 5: Jefferson City, MO

Cluster 6: Helena, MT; Lincoln, NE; Carson City, NV; Concord, NH; Trenton, NJ; Santa Fe, NM

Cluster 7: Albany, NY

Cluster 8: Raleigh, NC

Cluster 9: Bismarck, ND; Columbus, OH

Cluster 10: Oklahoma City, OK; Salem, OR; Harrisburg, PA; Providence, RI; Columbia, SC; Pierre, SD; Nashville, TN; Austin, TX

Cluster 11: Salt Lake City, UT; Montpelier, VT; Richmond, VA; Olympia, WA; Charleston, WV; Madison, WI

Cluster 12: Cheyenne, WY

I cannot find anything common in the clusters.  K-means is generated by assigning states to clusters randomly, calculate the centroid/mean of each cluster, and then assigning each state to the closest cluster.  It iterates between the two steps until the clusters do not change.  So, theoretically, what these clusters have in common are the closest to each other in terms of temperature.  

However, I don't equate Alaska temperatures being close to Alabama, Arizona, or Arkansas, so ....

I just noticed that the clusters are in alphabetical order.  Primary characteristic is the state alphabetical order.
</pre>

[
Summarize your k-means findings for 12 clusters.
]


## (15 points) Correlation

Correlation, like distance, is an important feature of multivariate analysis.  So let's review some basic correlation related to our climate data.  For simplicity, let's consider "Record High Temperature" and "Record Low Temperature" and see how they correlate with other factors we have gathered from Wikipedia.

Recall, in this table, "Jan-Dec" are different months of the same temperature variable.  

```{r, chunk-correlations-high, cache.rebuild=TRUE}

library(Hmisc); # p-values for correlation

high = subsetDataFrame(climate, c("key", "units"), "==", c("Record high F (C)",1));
high = merge(high, capitals, by=c("capital","state"));

high.X = high[,c(5:18,21)]; # numeric data
high.cor = round( cor(high.X), digits=2);
# high.cor.p = rcorr(as.matrix(high.X), type="pearson");  # p-values for statistical significance ... # str(high.cor.p);

# examine July (idx = 7)

as.data.frame( high.cor ) ; # so it will render nicely in RStudio

high.cor.july = high.cor[,7];
high.cor.july;

```

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --

July correlates to the other months by season. Positive correlation. Strength depends on how close the other months are to July.  Months flanking July have strongest correlation, months 6 months away from it have the least amount of correlation.

Latitude is north when positive and south when negative This correlation is a negative slope, but very weak.  Negative slope means that as the temperature rises, the latitude decreases south, closer to the equater. 

Longitude is east west, and is positive, but also very weak. It very weakly correlates that the eastern part of the United States are hotter.  But 0 is no correlation, so 0.02 indicates to me that longitude has little effect on max temperatures in July.

Population correlation is 0.31, which is positive and somewhat strong. Much stronger correlation to July temperatures than Latitude/Longitude.  It indicates that the larger the city, the hotter the temperature.  It's not very strong -- but I do think it is an indication that warmer cities tend to be more populated than colder capitals.

</pre>

Describe the correlation of July in "Record high F (C)" to the other numeric factors printed above. 

**x is correlated with y (0.00).  This correlation is positive/negative which means ...  This correlation is strong/weak because ... overall, this suggests ... **

- July perfectly correlates with July (1.00).  This correlation is positive and very strong.  This is because they are the same data.
- With the months, you can note each, or plot a trend showing them, and discussing them briefly as a trend.
- latitude is a measure of north/south, so be certain to apply the correlation value with some meaning.  be certain you know which direction is positive or negative to correctly interpret the sign of the correlation.
- longitude is a measure of east/west, so be certain ...
- population is the size of the city  

- intuitively, which months do you think correlate most with latitude for this data?  which correlate the least?  is the correlation always the same sign (positive/negative), or does it change? [You can use the dataframe output to do this analysis, or create your own subset]

```{r, chunk-correlations-low, cache.rebuild=TRUE}

library(Hmisc); # p-values for correlation

low = subsetDataFrame(climate, c("key", "units"), "==", c("Record low F (C)",1));
low = merge(low, capitals, by=c("capital","state"));

low.X = low[,c(5:18,21)]; # numeric data
low.cor = round( cor(low.X), digits=2);
# low.cor.p = rcorr(as.matrix(low.X), type="pearson");  # p-values for statistical significance ... # str(low.cor.p);

# examine Jan (idx = 1)

as.data.frame( low.cor ) ; # so it will render nicely in RStudio

low.cor.january = low.cor[,1];
low.cor.january;

```

Describe the correlation of January in "Record low F (C)" to the other numeric factors printed above. 


<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">
-- WRITE SOMETHING HERE --
Jan correlates to the other months by season as well. Positive correlation. Strength depends on how close the other months are to Jan.  Months flanking July have strongest correlation, months 6 months away from it have the least amount of correlation.  Interestingly, correlations for Jan/Jun record low is .80 which is stronger than Jun/Jan record high is 0.25.

Latitude is north when positive and south when negative This correlation is a negative slope, and quite strong.  Negative slope means that as the temperature gets colder, the latitude of capital increases south, closer to the equater. 

Longitude is east west, and is negative, but weaker than latitude but stronger than July record high correlation from above. It correlates that the eastern part of the United States are colder.

Population correlation is 0.34, which is positive and somewhat strong. It indicates that the larger the city, the colder the temperature.  It's not very strong -- but I do think it is an indication that colder cities tend to be more populated than hotter capitals...?  <-- That doesn't make sense from a layman's perspective.

Note: I am not sure of my latitude/longitude answers. I understand the strength and slope of the correlation, but how it translates to effect is hazier for me.

</pre>

Similar to "high" writeup, but for the "low" data.

## "So What" is DATA ANALYSIS?

In the social sciences (e.g., Karl Weick), the concept of "sense making" refers to "the process by which people give meaning to their collective experiences".  I have used this framework in my high-technology innovation research (See Figure 1 of <http://www.mshaffer.com/arizona/pdf/LoneGenius.pdf>, my rubric concept comes from learning-theory growth models:  Nascent, Adolescent, Mature.) 

This final topic is reflective:  we are thinking about how we think.

### Statistics

The syllabus defined statistics as "the discipline that concerns the collection, organization, analysis, interpretation and presentation of data." (See <https://en.wikipedia.org/wiki/Statistics>)

There are 5 elements mentioned:  collection, organization, analysis, interpretation, and presentation of "data".  Are those equally weighted?  That is, should we devote 20% of our time to each of those?  Now, consider the "analysis" stage.  I have suggested there are two camps:  exploratory and confirmatory data analysis.  Are those equally weighted?  That is, should we devote 50% of our time to each of those?  Now, in an "equally-likely" scenario, we would have.

```{r, chunk-conclusion-equally-likely, cache.rebuild=TRUE}

x = c(20,20,10,10,20,20);
x.labels = c("collection", "organization", "EDA analysis", "CDA analysis", "interpretation", "presentation");
x.colors = c("blue", "lightgreen", "green", "darkgreen", "orange", "red");

barplot(x, 
        col = x.colors,
        ylim = c(0, 20),
        ylab = "Proportion: (Sums to 100)",
        main="Statistics as the Study of 'Data'");


text(1.14* (1:6), par("usr")[3], col = x.colors, labels = x.labels, srt = 45, adj = c(1.1,1.1), xpd = TRUE, cex=.75);
```

### Data Analytics

<IMG src="http://md5.mshaffer.com/WSU_STATS419/_images_/data-analytics.png" style="border: 2px black solid;" />
<div>**Source: https://data-analytics.wsu.edu/197-2/ ** (Accessed October 2010) </div>

"Data analytics is the application of powerful new methods—drawn from computer science, mathematics and statistics, and domain sciences—to collect, curate, analyze, discover and communicate knowledge from 'big data'." <https://data-analytics.wsu.edu/> (Accessed October 2010)


#### Importance of 'Data'

I love data.  

I also love math/physics.  I also love exploratory data analysis.  I also love computational statistics or statistical computing <https://en.wikipedia.org/wiki/Computational_statistics>.  I also love thinking about developing the one graphic to summarize data most effectively.

#### Apprenticeship as Learning a Trade
The idea of sharing in the learning process is an important aspect of the apprenticeship model.  You are learning a trade (data analytics).  I have experience in this trade.  My job as the instructor is to provide you with a variety of "situated-learning" experiences To help you understand the nature of the trade.   This exam is an example of such an experience.

#### Tools of the Trade

Below are the core requirements for the data analytics program:

* Calculus and linear algebra (10 credits)
* Computer science fundamentals (11 credits)
* Machine learning and data management (9 credits)
* Statistics (15 credits)
* Data analytics introduction, ethics & project-focused * capstone experience (9 credits)

These are not the tools of the trade, but hopefully, they introduce you to key tools of the trade.  What exactly are tools of the trade? [You will have an opportunity to write a response below.]

#### Dimensional Reduction, an Axiomatic View

This video was recently shared with me that highlights some distinctions among persons practicing various forms of data analysis <https://www.youtube.com/watch?v=uHGlCi9jOWY>.  As an orthogonal projection, I would create two axes.  On the horizontal axis (x-axis), I would place "theory of data" to the left and "application of data" toward the right.  On the vertical axis (y-axis), I would place "care for data integrity" at the top and "less care for data integrity" at the bottom.

#### Skills of the Trade

As someone that is coming from industry, having hired young people like you out of Computer Science, Electrical-Computer Engineering, I have opinions related to skills of the trade.

* Can you acquire an appreciation for "data intimacy"?
* Can you track and document how data is curated?
* Can you track and document the analyses you perform?  Can you recreate them?  Do you have basic version-control protocols in place?
* Can you view data from multiple perspectives and synthesize those perspectives to identify the central them of the data?  Can you be objective?  Can you try and identify objective metrics to enlighten your understanding about the essence of data?
* Can you experiment with different visualizations in search of an optimal "one graphic" result?  Do you have practice using various visualization tools?  Can you comprehend which visualization tool is appropriate for messaging (communicating results) to a particular audience?
* Can you communicate and defend your findings to a particular audience?  Are your communications professional?  Is the final work product both simple and comprehensive:  simple in its summary findings and comprehensive in its ability to be replicated and audited as necessary.

## (20 points) YOUR OPINION OF DATA ANALYTICS 

[This is worth 20 points.

Specifically, address: 

(1) what proportion of "statistics" should be divided among: collection, organization, analysis, interpretation, and presentation of "data" ... providing a `barplot` of your opinion within your response would seem appropriate


```{r, chunk-conclusion-student-perspective, cache.rebuild=TRUE}

x = c(15,15,20,10,20,20);  ## change these values and discuss ...
x.labels = c("collection", "organization", "EDA analysis", "CDA analysis", "interpretation", "presentation");
x.colors = c("blue", "lightgreen", "green", "darkgreen", "orange", "red");

barplot(x, 
        col = x.colors,
        ylim = c(0, max(x)),
        ylab = "Proportion: (Sums to 100)",
        main="Statistics as the Study of 'Data'");


text(1.14* (1:6), par("usr")[3], col = x.colors, labels = x.labels, srt = 45, adj = c(1.1,1.1), xpd = TRUE, cex=.75);
```

(2) what tools of the trade should you be acquiring from the core courses?  how are you doing in that acquisition process (e.g., tool X is ... and right now I feel like my understanding/proficiency of tool X ... ) ... 

(3) utilize the provided `plot` script to place the coure-course categories on the proposed x-y graph related to analytics practice (Applied vs Theoretical) and care of data integrity (Great Care vs Little Care) ... Also place your personal assessment on the plot script provided

(3) evaluate your skill-level on the six "skills of the trade":  Emerging (Nascent), Developing (Adolescent), Mastering (Mature).  explain your evaluation and include other important skills you believe are relevant that are not included

(4) Any other comments you would like to share.

]

<pre style="font-weight: bold; font-family: monospace; white-space: pre-wrap; background-color: #717171; border: 2px solid #981e32;">


1) This one was hard for me, and depending on where I put an emphasis on, I would have weighted them slightly differently.  I think it's all important, and also the lines can be blurry between these categories. For example, "collection" and "organization" are closely aligned -- the more organized the data collection, the less time you have to organize it into a usable format.  EDA and CDA also go hand in hand -- perhaps in pure statistics, CDA would be more heavily weighted than EDA, however, even in "pure" statistics, objectivity is important, and EDA is more weighted with objectivity.  Interpretation goes with EDA, and then blurs into Presentation category.  Interpretation is needed as the result of analysis, and then the efficacy of that interpretation is dependent on the quality of presentation.  I kept Interpretation and Presentation as high as it was originally plotted because I think that this is the step that most people place the lowest value in, and while it isn't as "hard" as "coding" or "math", it is harder to do effectively (not for technical co-workers, but to effectively communicate your interpretation to the non-technical).  And while I know how importatnt "presentation" is, I do tend to spend less time on it (and my courses reflect this lack of emphasis as well) than other areas (such as math, programming, and statistics, all of which focus on collection to interpretation).  

2) I'm not sure what is a tool of the trade, but I see data analyst as a blend of computer programmer and statistician. In most industries, since most need to generate profits, a data analyst also needs to be able to connect information to the company in a way to either generate revenue, or save money. But at the end of the day, it's tied to money. The last is more esoteric, and would require some general knowledge of the specific industry -- several years or so working for a company would be able to provide that.

Tool:: Programming -->provided by Python and Statistics classes.  

Right now I feel like my understanding/proficiency of X is: developing.  I've taken several courses (2 in Java, 1 right now in Python, 2 right now in R). I may do terrible in 2 hour timed exams without the resources of the Internet to correct my syntax, but I have taken to Python and R far easier than I originally anticipated from Java, because the core concepts are there.  What took me surprise in the beginning of this course was how differently dataframes are processed and accessed vs arrays.  I'm still a little weak in OOP -- but so much better than I was a year ago. But with programming, there are times I dream about the assignment I can't solve. 

Tool::Statistics -->provided by statistics classes

Right now I feel like my understanding/proficiency of X is: emerging. Again, this is my first "real" semester of statistics. Up to this point, my classes have been in math and programming (not counting the MIS classes). I took Intro to Statistics for Science/Engineers (nothing like the Intro to Statistics classes I took (twice) many years ago btw!) over the summer, and that is it. How am I doing in the acquisition process...?  Not sure.  Right now, I feel like it's very difficult to teach statistics to students -- things can be contextual, and fluid. Still, I did like how this midterm was constructed -- was quite contextually appropriate for a data analytics major.  In my other classes, I'm to perform all these regression tests and then make comments on the values I get or the plots I generate and that is the hardest part for me.  I feel like I'm guessing all the time.  Maybe it's because I don't have as good of a background in statistics as I feel like I had in programming (where I had to take a specific progression of classes), vs statistics (where I had basically no progression).  

Tool:: Tie the two together along with some awesome visualizations and quality control --> provided by experience

Very low proficiency -- but projects and assignments dedicated to this arena will probably round me out.  Also, I've had a friend offer to show me Github but I never "had" to, so classes like this that force me to dip my toe (and walk me through on how to do it via lectures) are important!  But in general, I think a synthesis will happen best in an internship/job scenario.  Everyone I talk to says corporate jobs are nothing like their college degree courses.  

3) plot: I evaluated myself as such as how I imagine myself in the field. I assume I'll be working in a corporate environment, and possibly needing to figure out where my boundaries are in terms of data provenance and the company. I believe in data provenance, but I also am not arrogant enough to assume I will not be faced with some ethical choices.  A manager told my husband to have a line in the sand that he will not cross when it comes to ethics and company wants. He's told me about people who bend far too easily "yes-men" and the rare few that are so inflexible regarding their strict ethical code that they are not able to stay in a job for more than a year (which isn't as bad of a thing in the Bay Area as it might be in other areas but that's not the point...) Where do I stand?  I don't know.  I will find out.  Balance and compromise between ethics and reality....(but I will NOT be working for Facebook, even IF they would even hire me which I doubt, but will never find out as I will never apply there).  I do imagine that I will be more towards the applied data practice -- I do not like math, and I've done it enough to know I never will.  I have not done enough statistics yet to know whether I "like" it or not.  

It's hard for me to evaluate the core course categories in terms of data integrity.  In essence, I feel as if core courses (as not being human), do not care at all for data integrity and all should be scored as "little care for data integrity".  It is the human that cares or should care.  

4) An appreciation for "data intimacy": Emerging
Track and document how data is curated: Emerging
Track and document analyses? Reproducibility? Basic version-control protocols: Emerging
View data from multiple perspectives and synthesize to identify the central theme: Emerging
Experiment with different visualization in search of optimal graphics: Emerging
Presentation skills: Emerging

I evaluated myself as "emerging" for all six areas. Of all of these, I think I have a "mature" appreciation for data intimacy, but that none of is in in a concrete form -- which the following two skills (track and document data collection and analysis) seems to reflect. At this time, I have done very little statistical analyses, or collection. This semester is my first. This class is the first to discuss version control using Github, Dropbox, json files, etc. This is the first to discuss data collection --granted, you've ended up creating the code for the data scraping, but you are scraping from the Internet. My sister has mentioned that she's never scraped the Internet before -- her datasets are provided to her by her companies.  This is my primary reason for categorizing myself as emergent.

I feel that by the end of Spring 2021 (where I will be taking at least two more stats classes plus the next programming course, assuming I don't fail), I will feel more confident in describing my skills as developing!

However, as Albert Einstein said, "As our circle of knowledge expands, so does the circumference of darkness surrounding it."

5) I like the more philosophical/opinionated parts of this class. 

I agree with the "one graphic".  Not so much as "one", but my husband's (who is a systems engineer by trade but does enjoy data more than most engineers) opinion on the importance of a well-done graphic is, and how humans are able to process information visually faster than any other means.

I'm surprised at how little coding there is in this midterm, and how much analysis there is.  Granted, at this time, I have not done the clustering/correlation categories of this midterm. In some ways, this makes it an "easier" test, but unlike my math classes, which has a "right" answer, statistics (no matter how the YouTube video rapped that it was precisely driven unlike data science, is open to interpretation.  This fluidity makes it hard for me, like I am floundering in murky water.  Given enough times being thrust into situations like this through my statistics courses and future employment, I think I will start to be able to become more sure-footed.  It's good to be forced to interpret the analysis....because that's a very important part of the whole world of data analytics.

This is one of my pet peeves with statistics/data analysis.  The murkiness.  In calculus, there is a right answer.  In programming, there are clear objectives to meet -- how you get there can be varied but objectives are clear.  Even in the other statistics class taking right now, statistics is about "reading" different visualizations or interpreting p-values, t-test values, MSE errors, etc etc, and then, sometimes, it's by context. I wonder if there is no easy way to teach "how to interpret visualizations/values" because it's been rather confusing for me.  Perhaps it's just taking the time to practice enough....

This was an exhausting midterm despite the low level of coding in it.  Maybe it was the length/depth of it. I did enjoy the philosophical parts and I found the topics within interesting by tying the "real" world with it (using actual imdb data, indeed.com data, ability to customize the distance boxes to different locations, etc etc. (And I'm still not done -- I left the clustering and correlation part for last as I needed a "break" and did this section).

</pre>


```{r, chunk-conclusion-analysis-data, cache.rebuild=TRUE}

# x is -1 for perfectly theoretical
# x is 1 for perfectly applied

# y is -1 for no care whatsoever for data integrity
# y is 1 is perfect care for data integrity

########################### basic plot setup #####
plot(0,0, col="white", 
  ylim=c(-1.5,1.5), xlim=c(-1.5,1.5),
  xlab = "",
  ylab = "",
  xaxt = 'n', bty = 'n', yaxt = 'n',
  main = "Axiomatic Perspective on Practice/Care",
  );
segments(-1,0,1,0, col="#999999");
segments(0,-1,0,1, col="#999999");
text(-1.1,0, "Theoretical Data Practice", cex=0.5, srt = 90);
text(1.1,0, "Applied Data Practice", cex=0.5, srt = -90);
text(0,1.1, "Great Care for Data Integrity", cex=0.5, srt = 0);
text(0,-1.1, "Little Care for Data Integrity", cex=0.5, srt = 0);
########################### basic plot setup #####


########################### you can add elements here #####
## this point represents the professor's self-perception
points(0.75, 0.95, pch=20, col="blue");
text(0.75, 0.95, "Shaffer (self)", col="blue", cex=0.75, srt = 45, pos=3);

#############  TODO ###### ... maybe change color for each data point

#https://brand.wsu.edu/visual/colors/
#crimson = #981e32
# you need to change the x,y from 0,0 ...
# you can change col ... cex (font size), srt (angle), and pos = 1,2,3,4
# 
points(.85, .85, pch=20, col="#981e32");
text(0.85, 0.85, "Student (self)", col="#981e32", cex=0.75, srt = 45, pos=3);
# 
# ## evaluate the Course Categories of Tools of the Trade
# ## give them a score
# 
points(-.5, -.5, pch=20, col="#981e32");
text(-.5, -.5, "Math(s)", col="#981e32", cex=0.5, srt = 45, pos=3);
# 
# 
points(.5, -.5, pch=20, col="#981e32");
text(.5, -.5, "Computer Science", col="#981e32", cex=0.5, srt = 45, pos=3);
# 
points(.25, -.5, pch=20, col="#981e32");
text(.25, -.5, "Machine learning", col="#981e32", cex=0.5, srt = 45, pos=3);
# 
points(-.15, -.5, pch=20, col="#981e32");
text(-.15, -.5, "Statistics", col="#981e32", cex=0.5, srt = 45, pos=3);
# 
points(.75, -.5, pch=20, col="#981e32");
text(.75, -.50, "Data analytics", col="#981e32", cex=0.5, srt = 45, pos=3);
# 
# # You have a track (e.g., Business)
points(.9, -.5, pch=20, col="#981e32");
text(.9, -.50, "MIS", col="#981e32", cex=0.5, srt = 45, pos=3);

```

